{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General User Models","text":"<p>General User Models (GUMs) learn about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture the user's knowledge and preferences. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions.</p> <p>tl;dr Everything you do can be used to make your systems more context-aware.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>First, you'll need to install the GUM package. There are two ways to do it:</p> <p>Getting Started I: Installing the GUM package</p> pipFrom source <p>Great! Just pip install.</p> <pre><code>&gt; pip install -U gum-ai\n</code></pre> <pre><code>&gt; git clone git@github.com:GeneralUserModels/gum.git\n&gt; cd gum\n&gt; pip install --editable .\n</code></pre> <p>You can start a GUM server directly from the command line. </p> <p>Getting Started II: Starting a GUM server</p> <p>We recommend running this in a tmux or screen session to keep it alive.</p> Local LMs on a GPU server (recommended)OpenAI <p>First, install SGLang and launch its server with your LM.</p> <pre><code>&gt; pip install \"sglang[all]\"\n&gt; pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ \n\n&gt; # Launch model\n&gt; CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server ....\n\n&gt; # name of the model you launched\n&gt; export MODEL_NAME=\"model-org/model-name\"\n\n&gt; # your full name\n&gt; export USER_NAME=\"Full Name\"\n\n&gt; # point this to the GUM multimodal model\n&gt; export GUM_LM_API_BASE=\"base-url\"\n\n&gt; # (optionally) set an API key\n&gt; export GUM_LM_API_KEY=\"None\"\n</code></pre> <p>Alternatively, we recommend using SkyPilot to serve and run your own models on the cloud. You can use the following skypilot.yaml file in the repo. You'll need to replace the HuggingFace token (HF_TOKEN) with your own. By default, we use Qwen 2.5 VL 32B (AWQ quanitized). A single H100 (80GB) should give you good enough throughput.</p> <p>You can authenticate by setting the <code>OPENAI_API_KEY</code> and <code>USER_NAME</code> env variables.</p> <pre><code>&gt; export OPENAI_API_KEY=\"your-api-key-here\"\n&gt; export USER_NAME=\"Full Name\"\n</code></pre> <p>Required Permissions</p> <p>Go to \u201cSystem Settings\u201d \u2192 \u201cPrivacy &amp; Security\u201d \u2192 \u201cAccessibility\u201d \u2192 turn on the \u201cTerminal\u201d option. This is necessary for GUM to observe your interactions and build its model when launched via the Terminal.</p> <p>When you first run the GUM (below), your system may also prompt you to grant accessibility and screen recording permissions to the application. You may need to restart the process a few times as you grant these permissions.</p> <p>Start the GUM listening process up through the Terminal app:</p> <pre><code>&gt; gum\n</code></pre> <p>Once you're all done, go ahead and try querying your GUM to view propositions and observations. You can query for recent propositions by just passing the -q flag through the CLI.</p> <pre><code>&gt; gum -q\n</code></pre> <p>Output: <pre><code>Found 10 results:\n\nProposition: Omar Shaikh is currently actively engaged in developing and refining a Python application utilizing asynchronous programming with Observer patterns.\nReasoning: The transcription indicates that Omar is working with the file `gum.py`, which includes classes and methods for managing asynchronous tasks and observers, as seen in functions like `start_update_loop()` and `_update_loop()`. These methods suggest a focus on maintaining efficiency in processing updates from various observers, clearly indicative of software development practices related to asynchronous programming.\nConfidence: 9.00\nRelevance Score: 0.50\n--------------------------------------------------------------------------------\n\nProposition: Omar Shaikh deprioritizes longer or complex interactions with unnecessary dependencies, focusing on streamlined application performance instead.\nReasoning: From the transcript, he is primarily concerned with efficient resource management in `gum.py`, such as ensuring active tasks are being handled correctly, which is shown through his attentiveness to cleaning up resources in `async def __aexit__`. This behavior implies he prioritizes simplicity and efficiency over possible more complicated implementations.\nConfidence: 8.00\nRelevance Score: 0.50\n--------------------------------------------------------------------------------\n\nProposition: Omar Shaikh demonstrates a clear preference for using Google AI Studio for coding support and suggestions.\nReasoning: Evidence from the transcript highlights that Omar interacts with Google AI Studio to receive suggestions for Python code modifications, specifically aimed at improving the codebase he is working on. This indicates a strong reliance on Google AI Studio resources, reinforcing his inclination towards engaging AI-assisted coding tools in his work.\nConfidence: 8.00\nRelevance Score: 0.50\n\n[...]\n</code></pre></p> <p>Optionally, you can pass a query string and the number of results you want back (by default, 10). In the example below, I want to find things that are related to work on GUMs. </p> <pre><code>&gt; gum -q \"gum\" -l 10\n</code></pre> <p>Output: <pre><code>Found 10 results:\n\nProposition: Omar Shaikh is executing coding projects, specifically troubleshooting errors in his Python scripts, such as resolving 'TypeError: 'InstrumentedSet' object is not subscriptable' related to the GUM application.\nReasoning: The terminal logs reflecting the traceback error during execution of the `gum` script reveal that Omar is engaged in debugging activities within his coding project.\nConfidence: 9.00\nRelevance Score: 0.39\n--------------------------------------------------------------------------------\n\nProposition: Omar Shaikh is actively developing a Python-based application called 'gum'.\nReasoning: The terminal output shows that Omar is working within a project directory that includes `gum.py`, `models.py`, and other related files. This indicates a focused effort on a specific software project, likely centered on artificial intelligence given the context of named modules and functionality updates.\nConfidence: 9.00\nRelevance Score: 0.11\n\n[...]\n</code></pre></p>"},{"location":"#applications","title":"Applications","text":"<p>Once you're all set up, check out the tutorials here. There are a host of cool applications you can build atop of GUMs.</p> <p>Getting Started III: Querying GUMs with the API</p> <p>One of the main methods you'll use to interface with the GUM is the query function. It's exactly what the CLI calls under the hood. Simply pass your query in as a parameter (uses BM25 under the hood). The query takes many more arguments, which you can read about here.</p> <pre><code>import asyncio\nfrom gum import gum\n\ngum_instance = gum(\"Your Name\", model=\"gpt-4.1\")\n\nasync def main():\n    await gum_instance.connect_db()\n    print(await gum_instance.query(\"email\"))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>For example: you can set up an MCP that uses GUMs here.</p>"},{"location":"#under-the-hood","title":"Under the hood","text":""},{"location":"#observers-collect-raw-interaction-data","title":"Observers collect raw interaction data.","text":"<p>Observers are modular components that capture various user interactions: screen content, notifications, etc. Each observer operates independently, streaming its observations to the GUM core for processing. We implement a Screen observer as an example.</p>"},{"location":"#propositions-describe-inferences-made-about-the-user","title":"Propositions describe inferences made about the user.","text":"<p>The core of GUM is its proposition system, which transforms raw observations into structured knowledge. Each proposition carries a confidence score and connects to related information, continuously updating as new evidence arrives.</p>"},{"location":"AI_PROVIDERS/","title":"AI Provider Configuration","text":"<p>This document explains how to configure different AI providers for the GUM system.</p>"},{"location":"AI_PROVIDERS/#overview","title":"Overview","text":"<p>The GUM system uses a unified AI client that supports multiple providers for different tasks:</p> <ul> <li>Text Completion: Azure OpenAI (default) or OpenAI</li> <li>Vision Completion: OpenRouter (default)</li> </ul>"},{"location":"AI_PROVIDERS/#provider-configuration","title":"Provider Configuration","text":""},{"location":"AI_PROVIDERS/#text-providers","title":"Text Providers","text":""},{"location":"AI_PROVIDERS/#azure-openai-default","title":"Azure OpenAI (Default)","text":"<pre><code># Required environment variables\nexport AZURE_OPENAI_API_KEY=\"your-azure-api-key\"\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-15-preview\"\nexport AZURE_OPENAI_DEPLOYMENT=\"gpt-4o\"  # Optional, defaults to gpt-4o\n\n# Optional: Explicitly set text provider (defaults to azure)\nexport TEXT_PROVIDER=\"azure\"\n</code></pre>"},{"location":"AI_PROVIDERS/#openai","title":"OpenAI","text":"<pre><code># Required environment variables\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# Optional environment variables\nexport OPENAI_MODEL=\"gpt-4o\"  # Optional, defaults to gpt-4o\nexport OPENAI_API_BASE=\"https://api.openai.com/v1\"  # Optional, uses default\nexport OPENAI_ORGANIZATION=\"your-org-id\"  # Optional\n\n# Set text provider to OpenAI\nexport TEXT_PROVIDER=\"openai\"\n</code></pre>"},{"location":"AI_PROVIDERS/#vision-providers","title":"Vision Providers","text":""},{"location":"AI_PROVIDERS/#openrouter-default","title":"OpenRouter (Default)","text":"<pre><code># Required environment variables\nexport OPENROUTER_API_KEY=\"your-openrouter-api-key\"\n\n# Optional environment variables\nexport OPENROUTER_MODEL=\"qwen/qwen-2.5-vl-72b-instruct:free\"  # Optional, uses default\n\n# Optional: Explicitly set vision provider (defaults to openrouter)\nexport VISION_PROVIDER=\"openrouter\"\n</code></pre>"},{"location":"AI_PROVIDERS/#usage-examples","title":"Usage Examples","text":""},{"location":"AI_PROVIDERS/#using-azure-openai-for-text-default","title":"Using Azure OpenAI for Text (Default)","text":"<pre><code>import asyncio\nfrom gum import gum\nfrom gum.observers import Observer\n\nasync def main():\n    # No special configuration needed - Azure is the default\n    async with gum(\"username\", \"model\") as g:\n        # Your GUM code here\n        pass\n\nasyncio.run(main())\n</code></pre>"},{"location":"AI_PROVIDERS/#using-openai-for-text","title":"Using OpenAI for Text","text":"<pre><code>import asyncio\nimport os\nfrom gum import gum\nfrom gum.observers import Observer\n\nasync def main():\n    # Set OpenAI as text provider\n    os.environ[\"TEXT_PROVIDER\"] = \"openai\"\n\n    async with gum(\"username\", \"model\") as g:\n        # Your GUM code here\n        pass\n\nasyncio.run(main())\n</code></pre>"},{"location":"AI_PROVIDERS/#testing-different-providers","title":"Testing Different Providers","text":""},{"location":"AI_PROVIDERS/#test-openai-client","title":"Test OpenAI Client","text":"<pre><code>python test_openai_client.py\n</code></pre>"},{"location":"AI_PROVIDERS/#test-unified-client-with-different-providers","title":"Test Unified Client with Different Providers","text":"<pre><code># Test with Azure OpenAI (default)\npython -c \"import asyncio; from unified_ai_client import test_unified_client; asyncio.run(test_unified_client())\"\n\n# Test with OpenAI\nTEXT_PROVIDER=openai python -c \"import asyncio; from unified_ai_client import test_unified_client; asyncio.run(test_unified_client())\"\n</code></pre>"},{"location":"AI_PROVIDERS/#provider-features","title":"Provider Features","text":"Provider Text Completion Vision Completion Notes Azure OpenAI Enterprise-grade, requires Azure subscription OpenAI Direct OpenAI API, requires OpenAI account OpenRouter Multiple vision models, cost-effective"},{"location":"AI_PROVIDERS/#error-handling","title":"Error Handling","text":"<p>The unified client includes automatic retry logic with exponential backoff for transient errors. You can configure retry behavior:</p> <pre><code>from unified_ai_client import UnifiedAIClient\n\nclient = UnifiedAIClient(\n    max_retries=5,          # Maximum retry attempts\n    base_delay=2.0,         # Base delay in seconds\n    max_delay=120.0,        # Maximum delay between retries\n    backoff_factor=2.0,     # Exponential backoff multiplier\n    jitter_factor=0.1       # Random jitter to prevent thundering herd\n)\n</code></pre>"},{"location":"AI_PROVIDERS/#environment-variables-reference","title":"Environment Variables Reference","text":""},{"location":"AI_PROVIDERS/#azure-openai","title":"Azure OpenAI","text":"<ul> <li><code>AZURE_OPENAI_API_KEY</code> (required)</li> <li><code>AZURE_OPENAI_ENDPOINT</code> (required)</li> <li><code>AZURE_OPENAI_API_VERSION</code> (required)</li> <li><code>AZURE_OPENAI_DEPLOYMENT</code> (optional, defaults to \"gpt-4o\")</li> </ul>"},{"location":"AI_PROVIDERS/#openai_1","title":"OpenAI","text":"<ul> <li><code>OPENAI_API_KEY</code> (required)</li> <li><code>OPENAI_MODEL</code> (optional, defaults to \"gpt-4o\")</li> <li><code>OPENAI_API_BASE</code> (optional, defaults to \"https://api.openai.com/v1\")</li> <li><code>OPENAI_ORGANIZATION</code> (optional)</li> </ul>"},{"location":"AI_PROVIDERS/#openrouter","title":"OpenRouter","text":"<ul> <li><code>OPENROUTER_API_KEY</code> (required)</li> <li><code>OPENROUTER_MODEL</code> (optional, defaults to \"qwen/qwen-2.5-vl-72b-instruct:free\")</li> </ul>"},{"location":"AI_PROVIDERS/#provider-selection","title":"Provider Selection","text":"<ul> <li><code>TEXT_PROVIDER</code> (optional, \"azure\" or \"openai\", defaults to \"azure\")</li> <li><code>VISION_PROVIDER</code> (optional, \"openrouter\", defaults to \"openrouter\")</li> </ul>"},{"location":"AI_PROVIDERS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"AI_PROVIDERS/#common-issues","title":"Common Issues","text":"<ol> <li>Missing API Keys: Ensure all required environment variables are set</li> <li>Network Issues: Check firewall/proxy settings</li> <li>Rate Limits: The client includes automatic retry with backoff</li> <li>Model Availability: Verify the model name is correct for your provider</li> </ol>"},{"location":"AI_PROVIDERS/#debug-logging","title":"Debug Logging","text":"<p>Enable debug logging to troubleshoot issues:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>This will show detailed HTTP requests and responses for debugging.</p>"},{"location":"ENHANCED_ANALYSIS/","title":"Enhanced Analysis &amp; Prompts System","text":""},{"location":"ENHANCED_ANALYSIS/#overview","title":"Overview","text":"<p>The Enhanced Analysis &amp; Prompts System implements detailed bullet-point analysis format with precise timestamp correlation for both screen observer buffering and video processing batching. This system provides structured, actionable insights about user workflow patterns and productivity optimization.</p>"},{"location":"ENHANCED_ANALYSIS/#key-features","title":"Key Features","text":""},{"location":"ENHANCED_ANALYSIS/#1-detailed-bullet-point-analysis-format","title":"1. Detailed Bullet-Point Analysis Format","text":"<p>The AI response follows this exact structure:</p> <pre><code>WORKFLOW ANALYSIS (START_TIME - END_TIME)\n\n\u2022 Specific Problem Moments (exact timestamps)\nHH:MM:SS AM/PM: [Specific issue], [duration/impact]\nHH:MM:SS AM/PM: [Another issue], [resolution time]\n\n\u2022 Productivity Patterns\nPeak focus: [time range] ([activity description])\nDistraction trigger: [specific event] at [time]\nRecovery pattern: [time to regain focus]\n\n\u2022 Application Usage\nMost used: [App name] ([X.X minutes])\nContext switches: [number] times in [duration]\nSwitch cost: Average [X] seconds per switch\n\n\u2022 Behavioral Insights\n[Specific observation about user behavior]\n[Pattern identified with evidence]\n[Recommendation based on observed data]\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#2-precise-timestamp-correlation","title":"2. Precise Timestamp Correlation","text":"<ul> <li>HH:MM:SS Format: All timestamps use 24-hour format for precision</li> <li>Event Correlation: Mouse/keyboard events correlated with screen captures</li> <li>Duration Tracking: Exact duration and impact measurements</li> <li>Time Range Analysis: Start and end times for batch processing</li> </ul>"},{"location":"ENHANCED_ANALYSIS/#3-enhanced-prompt-engineering","title":"3. Enhanced Prompt Engineering","text":"<p>The system uses comprehensive prompts that include: - Event timeline with precise timestamps - Frame sequence information - Analysis requirements with exact format specifications - Guidelines for extracting actionable insights</p>"},{"location":"ENHANCED_ANALYSIS/#implementation-details","title":"Implementation Details","text":""},{"location":"ENHANCED_ANALYSIS/#screen-observer-integration","title":"Screen Observer Integration","text":""},{"location":"ENHANCED_ANALYSIS/#new-methods-added","title":"New Methods Added","text":"<ol> <li><code>_analyze_batch_with_detailed_insights()</code></li> <li>Processes batches of screen events with detailed analysis</li> <li>Provides structured bullet-point format</li> <li> <p>Includes timestamp correlation</p> </li> <li> <p><code>_process_monitor_events_fallback()</code></p> </li> <li>Fallback method using original processing approach</li> <li>Ensures system reliability</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#updated-methods","title":"Updated Methods","text":"<ol> <li><code>_process_monitor_events()</code></li> <li>Now uses detailed analysis format</li> <li>Includes timestamp calculation and correlation</li> <li>Maintains backward compatibility</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#video-processing-integration","title":"Video Processing Integration","text":""},{"location":"ENHANCED_ANALYSIS/#enhanced-functions","title":"Enhanced Functions","text":"<ol> <li><code>analyze_batch_with_detailed_insights()</code> (in controller.py)</li> <li>Core detailed analysis function</li> <li>Supports both screen and video data</li> <li> <p>Provides structured output format</p> </li> <li> <p><code>process_frames_in_batches()</code></p> </li> <li>Updated to use detailed analysis</li> <li>Includes timestamp correlation</li> <li> <p>Maintains parallel processing architecture</p> </li> <li> <p><code>encode_frame_to_base64()</code></p> </li> <li>Added timestamp and event type tracking</li> <li>Enables precise correlation</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#integration-points","title":"Integration Points","text":"<ol> <li><code>generate_video_insights()</code></li> <li>Accepts detailed_analysis parameter</li> <li>Integrates with existing proposition system</li> <li>Maintains compatibility with chat system</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#configuration","title":"Configuration","text":""},{"location":"ENHANCED_ANALYSIS/#screen-observer-configuration","title":"Screen Observer Configuration","text":"<pre><code>from gum.observers.screen import Screen\n\n# Create observer with enhanced analysis\nscreen_observer = Screen(\n    buffer_minutes=10,  # Buffer duration\n    debug=True,         # Enable debug logging\n    model_name=\"gpt-4o-mini\"  # AI model for analysis\n)\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#video-processing-configuration","title":"Video Processing Configuration","text":"<pre><code># Process video with enhanced analysis\nresults = await process_video_frames_parallel(\n    video_path=\"path/to/video.mp4\",\n    max_frames=10,\n    job_id=\"unique_job_id\"\n)\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#usage-examples","title":"Usage Examples","text":""},{"location":"ENHANCED_ANALYSIS/#screen-observer-usage","title":"Screen Observer Usage","text":"<pre><code># The enhanced analysis is automatically used when buffering\n# No changes needed to existing code\n\n# Check buffer status\nstatus = screen_observer.get_buffer_status()\nprint(f\"Buffer size: {status['buffer_size']}\")\nprint(f\"Time remaining: {status['time_remaining_seconds']} seconds\")\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#video-processing-usage","title":"Video Processing Usage","text":"<pre><code># Enhanced analysis is automatically applied to batch processing\n# Results include detailed bullet-point format\n\nframe_results = await process_video_frames_parallel(\"video.mp4\")\nfor result in frame_results:\n    if 'analysis' in result:\n        print(f\"Frame {result['frame_number']}: {result['analysis']}\")\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#manual-detailed-analysis","title":"Manual Detailed Analysis","text":"<pre><code>from controller import analyze_batch_with_detailed_insights\n\n# Create frame batch data\nframe_batch = [\n    {\n        \"frame_number\": 1,\n        \"timestamp\": 3600,  # 1 hour in seconds\n        \"event_type\": \"click\",\n        \"base64_data\": \"...\",\n        \"before_path\": \"/path/to/before.jpg\",\n        \"after_path\": \"/path/to/after.jpg\"\n    }\n]\n\n# Perform detailed analysis\ndetailed_analysis = await analyze_batch_with_detailed_insights(\n    frame_batch,\n    \"custom_batch\",\n    \"01:00:00\",\n    \"01:01:00\"\n)\n\n# Extract insights\nfor analysis in detailed_analysis['detailed_analyses']:\n    print(f\"Analysis: {analysis['analysis']}\")\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#benefits","title":"Benefits","text":""},{"location":"ENHANCED_ANALYSIS/#1-structured-insights","title":"1. Structured Insights","text":"<ul> <li>Consistent Format: All analyses follow the same bullet-point structure</li> <li>Actionable Information: Specific recommendations and patterns</li> <li>Easy Parsing: Structured format enables automated processing</li> </ul>"},{"location":"ENHANCED_ANALYSIS/#2-precise-timing","title":"2. Precise Timing","text":"<ul> <li>Exact Timestamps: HH:MM:SS format for precise correlation</li> <li>Duration Tracking: Measure impact and resolution times</li> <li>Pattern Recognition: Identify productivity patterns over time</li> </ul>"},{"location":"ENHANCED_ANALYSIS/#3-enhanced-context","title":"3. Enhanced Context","text":"<ul> <li>Batch Processing: Multiple events analyzed together for better context</li> <li>Historical Correlation: Events analyzed in sequence</li> <li>Cross-Platform: Works with both screen and video data</li> </ul>"},{"location":"ENHANCED_ANALYSIS/#4-cost-optimization","title":"4. Cost Optimization","text":"<ul> <li>Batch Analysis: Reduces API calls through intelligent batching</li> <li>Context-Aware: Better analysis quality with batch context</li> <li>Efficient Processing: Parallel processing with semaphore control</li> </ul>"},{"location":"ENHANCED_ANALYSIS/#error-handling","title":"Error Handling","text":""},{"location":"ENHANCED_ANALYSIS/#fallback-mechanisms","title":"Fallback Mechanisms","text":"<ol> <li>Screen Observer Fallback</li> <li>If detailed analysis fails, falls back to original processing</li> <li>Maintains system reliability</li> <li> <p>Logs errors for debugging</p> </li> <li> <p>Video Processing Fallback</p> </li> <li>Individual frame processing if batch fails</li> <li>Graceful degradation</li> <li>Error reporting and recovery</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#error-types","title":"Error Types","text":"<ol> <li>API Errors: Authentication, rate limiting, network issues</li> <li>Processing Errors: Invalid data, encoding issues</li> <li>System Errors: Memory, file system issues</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#testing","title":"Testing","text":""},{"location":"ENHANCED_ANALYSIS/#test-suite","title":"Test Suite","text":"<p>Run the comprehensive test suite:</p> <pre><code>python test_enhanced_analysis.py\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#test-coverage","title":"Test Coverage","text":"<ol> <li>Screen Observer Tests</li> <li>Buffer management</li> <li>Detailed analysis processing</li> <li> <p>Fallback mechanisms</p> </li> <li> <p>Video Processing Tests</p> </li> <li>Batch processing</li> <li>Timestamp correlation</li> <li> <p>Format validation</p> </li> <li> <p>Integration Tests</p> </li> <li>System compatibility</li> <li>API integration</li> <li>Error handling</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#migration-guide","title":"Migration Guide","text":""},{"location":"ENHANCED_ANALYSIS/#from-previous-versions","title":"From Previous Versions","text":"<ol> <li>No Breaking Changes: Existing code continues to work</li> <li>Automatic Enhancement: New format applied automatically</li> <li>Optional Features: Can be disabled if needed</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#configuration-updates","title":"Configuration Updates","text":"<ol> <li>Screen Observer: No changes required</li> <li>Video Processing: No changes required</li> <li>API Endpoints: No changes required</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ENHANCED_ANALYSIS/#common-issues","title":"Common Issues","text":"<ol> <li>API Authentication Errors</li> <li>Check API key configuration</li> <li>Verify rate limits</li> <li> <p>Check network connectivity</p> </li> <li> <p>Format Issues</p> </li> <li>Verify prompt structure</li> <li>Check timestamp format</li> <li> <p>Validate input data</p> </li> <li> <p>Performance Issues</p> </li> <li>Adjust batch sizes</li> <li>Monitor memory usage</li> <li>Check semaphore limits</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for detailed information:</p> <pre><code># Screen observer\nscreen_observer = Screen(debug=True)\n\n# Video processing\nlogging.getLogger().setLevel(logging.DEBUG)\n</code></pre>"},{"location":"ENHANCED_ANALYSIS/#future-enhancements","title":"Future Enhancements","text":""},{"location":"ENHANCED_ANALYSIS/#planned-features","title":"Planned Features","text":"<ol> <li>Advanced Pattern Recognition</li> <li>Machine learning integration</li> <li>Predictive analytics</li> <li> <p>Behavioral modeling</p> </li> <li> <p>Enhanced Visualization</p> </li> <li>Timeline visualization</li> <li>Pattern charts</li> <li> <p>Productivity metrics</p> </li> <li> <p>Real-time Analysis</p> </li> <li>Live streaming analysis</li> <li>Instant feedback</li> <li>Adaptive prompts</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#api-extensions","title":"API Extensions","text":"<ol> <li>Custom Format Support</li> <li>User-defined analysis formats</li> <li>Template system</li> <li> <p>Format validation</p> </li> <li> <p>Advanced Correlation</p> </li> <li>Multi-modal data fusion</li> <li>Cross-device correlation</li> <li>Environmental factors</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#support","title":"Support","text":"<p>For issues and questions:</p> <ol> <li>Documentation: Check this guide and API documentation</li> <li>Testing: Run test suite to verify functionality</li> <li>Logs: Enable debug mode for detailed information</li> <li>Fallback: System includes robust fallback mechanisms</li> </ol>"},{"location":"ENHANCED_ANALYSIS/#conclusion","title":"Conclusion","text":"<p>The Enhanced Analysis &amp; Prompts System provides structured, actionable insights with precise timestamp correlation while maintaining full backward compatibility. The system automatically applies enhanced analysis to both screen observer buffering and video processing, delivering better quality insights with improved cost efficiency. </p>"},{"location":"FRONTEND_INTEGRATION/","title":"Frontend Integration - Enhanced Analysis Format","text":""},{"location":"FRONTEND_INTEGRATION/#overview","title":"Overview","text":"<p>This document describes the frontend integration for the enhanced bullet-point analysis format implemented in Phase 5. The integration provides a comprehensive display system for timestamped workflow insights while maintaining full backward compatibility with existing functionality.</p>"},{"location":"FRONTEND_INTEGRATION/#features-implemented","title":"Features Implemented","text":""},{"location":"FRONTEND_INTEGRATION/#1-enhanced-results-display","title":"1. Enhanced Results Display","text":""},{"location":"FRONTEND_INTEGRATION/#new-detailed-analysis-card","title":"New Detailed Analysis Card","text":"<ul> <li>Location: <code>frontend/static/js/app.js</code> - <code>generateDetailedAnalysisCard()</code></li> <li>Purpose: Displays structured bullet-point insights with timestamp correlation</li> <li>Features:</li> <li>Time range display with frame count and batch ID</li> <li>Categorized insights (Problem Moments, Productivity Patterns, Application Usage, Behavioral Insights)</li> <li>Color-coded categories for easy identification</li> <li>Responsive grid layout</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#insight-categories","title":"Insight Categories","text":"<ul> <li>Problem Moments: Red-themed with timestamp highlighting</li> <li>Productivity Patterns: Green-themed for positive patterns</li> <li>Application Usage: Blue-themed for usage statistics</li> <li>Behavioral Insights: Orange-themed for behavioral observations</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#2-enhanced-chat-interface","title":"2. Enhanced Chat Interface","text":""},{"location":"FRONTEND_INTEGRATION/#timestamp-highlighting","title":"Timestamp Highlighting","text":"<ul> <li>Location: <code>frontend/static/js/chat.js</code> - <code>formatMessage()</code></li> <li>Features:</li> <li>Automatic detection of HH:MM:SS timestamps</li> <li>Highlighted display with monospace font</li> <li>Brand color theming</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#bullet-point-styling","title":"Bullet Point Styling","text":"<ul> <li>Features:</li> <li>Styled bullet points with brand color</li> <li>Workflow term highlighting (Peak focus, Distraction trigger, etc.)</li> <li>Enhanced readability for structured content</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#updated-chat-suggestions","title":"Updated Chat Suggestions","text":"<ul> <li>New Suggestions:</li> <li>\"Problem moments today\"</li> <li>\"Context switches\"</li> <li>Enhanced welcome message with timestamp capabilities</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#3-responsive-design","title":"3. Responsive Design","text":""},{"location":"FRONTEND_INTEGRATION/#mobile-optimization","title":"Mobile Optimization","text":"<ul> <li>Grid Layout: Single column on mobile devices</li> <li>Touch-Friendly: Optimized spacing for touch interfaces</li> <li>Readable Text: Adjusted font sizes for mobile screens</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#dark-mode-support","title":"Dark Mode Support","text":"<ul> <li>Consistent Theming: All new components support dark mode</li> <li>Color Adaptation: Proper contrast in both light and dark themes</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#technical-implementation","title":"Technical Implementation","text":""},{"location":"FRONTEND_INTEGRATION/#javascript-functions-added","title":"JavaScript Functions Added","text":""},{"location":"FRONTEND_INTEGRATION/#generatedetailedanalysiscardresults","title":"<code>generateDetailedAnalysisCard(results)</code>","text":"<pre><code>// Generates the detailed analysis card with bullet-point insights\ngenerateDetailedAnalysisCard(results) {\n    const detailedAnalysis = results.detailed_analysis;\n    if (!detailedAnalysis) return '';\n\n    const timeRange = detailedAnalysis.time_range || 'Unknown Time Range';\n    const frameCount = detailedAnalysis.frame_count || 0;\n    const batchId = detailedAnalysis.batch_id || 'Unknown Batch';\n\n    return `\n        &lt;div class=\"result-card full-width detailed-analysis-card\"&gt;\n            &lt;div class=\"result-header\"&gt;\n                &lt;h3&gt;&lt;i class=\"fas fa-clock\"&gt;&lt;/i&gt; Workflow Analysis (${timeRange})&lt;/h3&gt;\n                &lt;div class=\"analysis-meta\"&gt;\n                    &lt;span class=\"meta-item\"&gt;&lt;i class=\"fas fa-layer-group\"&gt;&lt;/i&gt; ${frameCount} frames&lt;/span&gt;\n                    &lt;span class=\"meta-item\"&gt;&lt;i class=\"fas fa-tag\"&gt;&lt;/i&gt; ${batchId}&lt;/span&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;div class=\"result-content\"&gt;\n                &lt;div class=\"detailed-insights\"&gt;\n                    ${this.generateDetailedInsights(detailedAnalysis)}\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    `;\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#generatedetailedinsightsdetailedanalysis","title":"<code>generateDetailedInsights(detailedAnalysis)</code>","text":"<pre><code>// Processes detailed analysis and categorizes insights\ngenerateDetailedInsights(detailedAnalysis) {\n    const insights = {\n        problemMoments: [],\n        productivityPatterns: [],\n        applicationUsage: [],\n        behavioralInsights: []\n    };\n\n    // Extract and categorize insights from analysis content\n    // Returns structured HTML for each category\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#formatinsighttexttext","title":"<code>formatInsightText(text)</code>","text":"<pre><code>// Formats insight text with timestamp highlighting and styling\nformatInsightText(text) {\n    // Highlight timestamps\n    text = text.replace(/(\\d{2}:\\d{2}:\\d{2})/g, '&lt;span class=\"timestamp\"&gt;$1&lt;/span&gt;');\n\n    // Highlight key terms\n    text = text.replace(/(Peak focus|Distraction trigger|Recovery pattern|Most used|Context switches|Switch cost)/g, '&lt;strong&gt;$1&lt;/strong&gt;');\n\n    return text;\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#css-classes-added","title":"CSS Classes Added","text":""},{"location":"FRONTEND_INTEGRATION/#analysis-display","title":"Analysis Display","text":"<pre><code>.detailed-analysis-card {\n    border-left: 4px solid var(--brand-cyan);\n}\n\n.insights-categories {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n    gap: 20px;\n}\n\n.insight-category {\n    background: var(--bg-secondary);\n    border-radius: 8px;\n    padding: 16px;\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#category-specific-styling","title":"Category-Specific Styling","text":"<pre><code>.problem-moments .insight-item {\n    border-left-color: var(--error-color);\n}\n\n.productivity-patterns .insight-item {\n    border-left-color: var(--success-color);\n}\n\n.application-usage .insight-item {\n    border-left-color: var(--info-color);\n}\n\n.behavioral-insights .insight-item {\n    border-left-color: var(--warning-color);\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#chat-formatting","title":"Chat Formatting","text":"<pre><code>.timestamp-highlight {\n    background: var(--brand-cyan);\n    color: white;\n    padding: 2px 6px;\n    border-radius: 4px;\n    font-family: var(--font-family-mono);\n    font-size: 11px;\n    font-weight: 600;\n}\n\n.bullet-point {\n    color: var(--brand-cyan);\n    font-weight: bold;\n    font-size: 14px;\n    margin-right: 4px;\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#data-structure","title":"Data Structure","text":""},{"location":"FRONTEND_INTEGRATION/#expected-detailed-analysis-format","title":"Expected Detailed Analysis Format","text":"<pre><code>{\n    \"detailed_analysis\": {\n        \"batch_id\": \"batch_001\",\n        \"time_range\": \"09:30:00 - 10:15:00\",\n        \"frame_count\": 5,\n        \"detailed_analyses\": [\n            {\n                \"frame_number\": 1,\n                \"timestamp\": \"09:30:15\",\n                \"event_type\": \"screen_capture\",\n                \"analysis\": \"WORKFLOW ANALYSIS (09:30:00 - 10:15:00)\\n\\n\u2022 Specific Problem Moments (exact timestamps)\\n09:30:15: Email notification distraction, 3 minutes lost\\n\\n\u2022 Productivity Patterns\\nPeak focus: 09:45:00 - 10:00:00 (Deep work on project)\\n\\n\u2022 Application Usage\\nMost used: Code Editor (45.2 minutes)\\n\\n\u2022 Behavioral Insights\\nUser shows strong focus recovery after distractions\",\n                \"base64_data\": \"...\",\n                \"batch_processed\": true,\n                \"batch_id\": \"batch_001\"\n            }\n        ],\n        \"summary\": {\n            \"start_time\": \"09:30:00\",\n            \"end_time\": \"10:15:00\",\n            \"total_duration\": 45,\n            \"event_types\": [\"screen_capture\", \"mouse_event\", \"keyboard_event\"]\n        }\n    }\n}\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#usage-examples","title":"Usage Examples","text":""},{"location":"FRONTEND_INTEGRATION/#displaying-enhanced-results","title":"Displaying Enhanced Results","text":"<pre><code>// The enhanced results are automatically displayed when analysis results contain detailed_analysis\nconst results = {\n    processing_time_ms: 1500,\n    frames_analyzed: 5,\n    summary: \"Analysis completed successfully\",\n    detailed_analysis: {\n        // ... detailed analysis data\n    }\n};\n\n// Results are automatically formatted with the new bullet-point display\napp.displayResults(results);\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#chat-integration","title":"Chat Integration","text":"<pre><code>// Chat automatically formats timestamps and bullet points\nconst message = \"At 09:30:15, you had a distraction that lasted 3 minutes. \u2022 Peak focus was between 09:45:00 - 10:00:00\";\n\n// Automatically formatted with:\n// - Timestamps highlighted: &lt;span class=\"timestamp-highlight\"&gt;09:30:15&lt;/span&gt;\n// - Bullet points styled: &lt;span class=\"bullet-point\"&gt;\u2022&lt;/span&gt;\n// - Workflow terms emphasized: &lt;strong class=\"workflow-term\"&gt;Peak focus&lt;/strong&gt;\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#testing","title":"Testing","text":""},{"location":"FRONTEND_INTEGRATION/#frontend-integration-test-suite","title":"Frontend Integration Test Suite","text":"<p>Run the comprehensive test suite to verify all features:</p> <pre><code>python test_frontend_integration.py\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#test-coverage","title":"Test Coverage","text":"<ul> <li>\u2705 Detailed analysis structure validation</li> <li>\u2705 Timestamp format verification (HH:MM:SS)</li> <li>\u2705 Bullet-point format extraction</li> <li>\u2705 Problem moments extraction with timestamps</li> <li>\u2705 Productivity patterns extraction</li> <li>\u2705 Application usage data extraction</li> <li>\u2705 Behavioral insights extraction</li> <li>\u2705 Frontend data compatibility (JSON serialization)</li> <li>\u2705 Chat integration format validation</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#backward-compatibility","title":"Backward Compatibility","text":""},{"location":"FRONTEND_INTEGRATION/#existing-functionality-preserved","title":"Existing Functionality Preserved","text":"<ul> <li>\u2705 All existing API endpoints unchanged</li> <li>\u2705 Current results display still works</li> <li>\u2705 Chat functionality maintained</li> <li>\u2705 Export capabilities preserved</li> <li>\u2705 Theme switching works with new components</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#graceful-degradation","title":"Graceful Degradation","text":"<ul> <li>If <code>detailed_analysis</code> is not present in results, the enhanced card is not displayed</li> <li>Existing results continue to show in the original format</li> <li>No breaking changes to existing interfaces</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#performance-considerations","title":"Performance Considerations","text":""},{"location":"FRONTEND_INTEGRATION/#optimization-features","title":"Optimization Features","text":"<ul> <li>Lazy Loading: Detailed analysis card only renders when data is available</li> <li>Efficient DOM Updates: Minimal re-rendering with targeted updates</li> <li>Memory Management: Automatic cleanup of event listeners and timers</li> <li>Responsive Images: Optimized image handling for different screen sizes</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#browser-compatibility","title":"Browser Compatibility","text":"<ul> <li>Modern Browsers: Full support for CSS Grid and modern JavaScript features</li> <li>Fallbacks: Graceful degradation for older browsers</li> <li>Mobile Support: Optimized for touch interfaces and mobile browsers</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#future-enhancements","title":"Future Enhancements","text":""},{"location":"FRONTEND_INTEGRATION/#planned-features","title":"Planned Features","text":"<ol> <li>Interactive Timestamps: Clickable timestamps that jump to specific moments</li> <li>Export Options: Enhanced export with detailed analysis formatting</li> <li>Filtering: Filter insights by category or time range</li> <li>Search: Search within detailed analysis content</li> <li>Charts: Visual representation of productivity patterns</li> </ol>"},{"location":"FRONTEND_INTEGRATION/#customization-options","title":"Customization Options","text":"<ol> <li>Theme Customization: User-configurable color schemes</li> <li>Layout Preferences: Adjustable grid layouts and card sizes</li> <li>Content Filtering: Show/hide specific insight categories</li> <li>Timestamp Format: Configurable timestamp display formats</li> </ol>"},{"location":"FRONTEND_INTEGRATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"FRONTEND_INTEGRATION/#common-issues","title":"Common Issues","text":""},{"location":"FRONTEND_INTEGRATION/#detailed-analysis-not-displaying","title":"Detailed Analysis Not Displaying","text":"<ul> <li>Cause: Missing <code>detailed_analysis</code> field in results</li> <li>Solution: Ensure backend is returning detailed analysis data</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#timestamps-not-highlighting","title":"Timestamps Not Highlighting","text":"<ul> <li>Cause: Incorrect timestamp format</li> <li>Solution: Verify timestamps are in HH:MM:SS format</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#styling-issues","title":"Styling Issues","text":"<ul> <li>Cause: CSS not loading or theme conflicts</li> <li>Solution: Check CSS file paths and theme compatibility</li> </ul>"},{"location":"FRONTEND_INTEGRATION/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging to troubleshoot issues:</p> <pre><code>// In browser console\nlocalStorage.setItem('gum-debug', 'true');\n// Refresh page to see detailed logging\n</code></pre>"},{"location":"FRONTEND_INTEGRATION/#conclusion","title":"Conclusion","text":"<p>The frontend integration successfully provides a comprehensive display system for the enhanced bullet-point analysis format while maintaining full backward compatibility. The implementation includes:</p> <ul> <li>\u2705 Enhanced results display with categorized insights</li> <li>\u2705 Improved chat interface with timestamp highlighting</li> <li>\u2705 Responsive design for all screen sizes</li> <li>\u2705 Dark mode support</li> <li>\u2705 Comprehensive testing suite</li> <li>\u2705 Zero breaking changes to existing functionality</li> </ul> <p>The system is production-ready and provides users with detailed, actionable insights about their workflow patterns with precise timestamp correlation. </p>"},{"location":"RATE_LIMITING/","title":"GUM API Rate Limiting Implementation","text":""},{"location":"RATE_LIMITING/#overview","title":"Overview","text":"<p>The GUM application now includes a comprehensive, production-ready rate limiting system that protects API endpoints from abuse while providing excellent user experience through intelligent frontend handling.</p>"},{"location":"RATE_LIMITING/#features","title":"Features","text":""},{"location":"RATE_LIMITING/#backend-rate-limiting","title":"Backend Rate Limiting","text":"<ul> <li>Per-endpoint limits: Configurable limits for different API endpoints</li> <li>Memory-efficient: Automatic cleanup of old requests with configurable memory limits</li> <li>Thread-safe: Concurrent request handling without race conditions</li> <li>Production-ready: Background cleanup worker and comprehensive logging</li> <li>Monitoring: Built-in statistics and admin endpoints for monitoring</li> </ul>"},{"location":"RATE_LIMITING/#frontend-rate-limiting","title":"Frontend Rate Limiting","text":"<ul> <li>Graceful handling: User-friendly 429 response handling</li> <li>Countdown timers: Real-time countdown on disabled buttons</li> <li>Visual feedback: Progress indicators and status displays</li> <li>Auto-recovery: Automatic re-enabling when limits reset</li> <li>Comprehensive coverage: Handles all rate-limited endpoints</li> </ul>"},{"location":"RATE_LIMITING/#configuration","title":"Configuration","text":""},{"location":"RATE_LIMITING/#default-rate-limits","title":"Default Rate Limits","text":"Endpoint Limit Window Description <code>/observations/video</code> 5 requests 5 minutes Video uploads (resource-intensive) <code>/observations/text</code> 20 requests 1 minute Text submissions <code>/query</code> 30 requests 1 minute Search queries <code>default</code> 100 requests 1 minute All other endpoints"},{"location":"RATE_LIMITING/#customizing-rate-limits","title":"Customizing Rate Limits","text":"<p>You can configure custom rate limits by modifying the <code>RATE_LIMITS</code> dictionary in <code>controller.py</code>:</p> <pre><code>RATE_LIMITS = {\n    \"/observations/video\": (5, 300),    # 5 videos per 5 minutes\n    \"/observations/text\": (20, 60),     # 20 text submissions per minute\n    \"/query\": (30, 60),                 # 30 queries per minute\n    \"default\": (100, 60)                # 100 requests per minute for other endpoints\n}\n</code></pre>"},{"location":"RATE_LIMITING/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more advanced configuration, you can use the rate limiter's configuration methods:</p> <pre><code>from rate_limiter import rate_limiter\n\n# Configure a custom endpoint with advanced options\nrate_limiter.configure_endpoint(\n    endpoint=\"/custom/endpoint\",\n    max_requests=50,\n    window_seconds=120,\n    cleanup_interval=300,      # Cleanup every 5 minutes\n    max_memory_entries=5000    # Keep max 5000 entries in memory\n)\n</code></pre>"},{"location":"RATE_LIMITING/#implementation-details","title":"Implementation Details","text":""},{"location":"RATE_LIMITING/#backend-architecture","title":"Backend Architecture","text":""},{"location":"RATE_LIMITING/#rate-limiter-class-rate_limiterpy","title":"Rate Limiter Class (<code>rate_limiter.py</code>)","text":"<ul> <li>Thread-safe: Uses <code>threading.RLock()</code> for concurrent access</li> <li>Memory management: Automatic cleanup of old requests</li> <li>Background worker: Dedicated thread for cleanup operations</li> <li>Statistics tracking: Comprehensive metrics for monitoring</li> </ul>"},{"location":"RATE_LIMITING/#middleware-controllerpy","title":"Middleware (<code>controller.py</code>)","text":"<ul> <li>Universal coverage: Applies to all endpoints automatically</li> <li>Header injection: Adds rate limit headers to all responses</li> <li>Logging: Detailed logging of rate limit violations</li> <li>Exclusions: Skips health checks and static files</li> </ul>"},{"location":"RATE_LIMITING/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"RATE_LIMITING/#rate-limit-handler-appjs","title":"Rate Limit Handler (<code>app.js</code>)","text":"<ul> <li>Endpoint tracking: Monitors rate limits for all endpoints</li> <li>Countdown timers: Real-time countdown with automatic cleanup</li> <li>Visual indicators: Progress bars and status displays</li> <li>Auto-recovery: Automatic re-enabling when limits reset</li> </ul>"},{"location":"RATE_LIMITING/#ui-components-stylescss","title":"UI Components (<code>styles.css</code>)","text":"<ul> <li>Rate-limited buttons: Distinct styling for disabled state</li> <li>Progress indicators: Visual feedback for remaining requests</li> <li>Animations: Smooth transitions and countdown effects</li> <li>Dark mode support: Consistent theming across modes</li> </ul>"},{"location":"RATE_LIMITING/#api-endpoints","title":"API Endpoints","text":""},{"location":"RATE_LIMITING/#rate-limit-headers","title":"Rate Limit Headers","text":"<p>All API responses include rate limit headers:</p> <pre><code>X-RateLimit-Limit: 20\nX-RateLimit-Remaining: 15\nX-RateLimit-Reset: 1640995200\n</code></pre>"},{"location":"RATE_LIMITING/#admin-endpoints","title":"Admin Endpoints","text":""},{"location":"RATE_LIMITING/#get-rate-limit-statistics","title":"Get Rate Limit Statistics","text":"<pre><code>GET /admin/rate-limits\n</code></pre> <p>Response: <pre><code>{\n  \"global_stats\": {\n    \"total_requests\": 1250,\n    \"rate_limited_requests\": 23,\n    \"cleanup_runs\": 45,\n    \"last_cleanup\": 1640995200.0,\n    \"total_endpoints\": 4,\n    \"total_entries\": 156,\n    \"rate_limit_percentage\": 1.84\n  },\n  \"endpoint_stats\": {\n    \"/observations/video\": {\n      \"endpoint\": \"/observations/video\",\n      \"current_requests\": 3,\n      \"max_requests\": 5,\n      \"window_seconds\": 300,\n      \"remaining_requests\": 2,\n      \"reset_time\": 1640995500.0,\n      \"is_limited\": false\n    }\n  },\n  \"timestamp\": \"2022-01-01T12:00:00\"\n}\n</code></pre></p>"},{"location":"RATE_LIMITING/#reset-rate-limits","title":"Reset Rate Limits","text":"<pre><code>POST /admin/rate-limits/reset?endpoint=/observations/video\n</code></pre> <p>Response: <pre><code>{\n  \"message\": \"Rate limits reset for /observations/video\",\n  \"endpoint\": \"/observations/video\"\n}\n</code></pre></p>"},{"location":"RATE_LIMITING/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"RATE_LIMITING/#log-messages","title":"Log Messages","text":"<p>The system logs various events for monitoring:</p> <pre><code>INFO: Configured rate limit for /observations/video: 5 requests per 300s\nWARNING: Rate limit exceeded for /observations/video: 5/5 requests in 300s window. Reset in 245s. Client IP: 192.168.1.100\nINFO: High usage for /query: 4 requests remaining out of 30\nINFO: Cleanup completed: 23 requests, 2 endpoints removed\n</code></pre>"},{"location":"RATE_LIMITING/#metrics","title":"Metrics","text":"<p>Key metrics tracked: - Total requests processed - Rate-limited requests count - Cleanup operations performed - Memory usage (total entries) - Rate limit violation percentage</p>"},{"location":"RATE_LIMITING/#frontend-usage","title":"Frontend Usage","text":""},{"location":"RATE_LIMITING/#rate-limit-status","title":"Rate Limit Status","text":"<p>Check if an endpoint is rate limited:</p> <pre><code>if (gumApp.isRateLimited('/observations/video')) {\n    console.log('Video upload is rate limited');\n}\n</code></pre>"},{"location":"RATE_LIMITING/#get-rate-limit-information","title":"Get Rate Limit Information","text":"<pre><code>const status = gumApp.getRateLimitStatus('/observations/video');\nif (status) {\n    console.log(`Reset in ${Math.ceil((status.resetTime - Date.now()) / 1000)}s`);\n}\n</code></pre>"},{"location":"RATE_LIMITING/#error-handling","title":"Error Handling","text":""},{"location":"RATE_LIMITING/#429-response-format","title":"429 Response Format","text":"<p>When rate limits are exceeded, the API returns:</p> <pre><code>HTTP/1.1 429 Too Many Requests\nRetry-After: 245\nX-RateLimit-Limit: 5\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1640995500\n\n{\n  \"detail\": \"Rate limit exceeded. Try again in 245 seconds.\"\n}\n</code></pre>"},{"location":"RATE_LIMITING/#frontend-error-handling","title":"Frontend Error Handling","text":"<p>The frontend automatically: 1. Detects 429 responses 2. Extracts rate limit information 3. Shows user-friendly toast notifications 4. Disables affected buttons with countdown 5. Automatically re-enables when limits reset</p>"},{"location":"RATE_LIMITING/#performance-considerations","title":"Performance Considerations","text":""},{"location":"RATE_LIMITING/#memory-management","title":"Memory Management","text":"<ul> <li>Automatic cleanup of old requests every minute</li> <li>Configurable maximum memory entries per endpoint</li> <li>Removal of empty endpoints to free memory</li> </ul>"},{"location":"RATE_LIMITING/#thread-safety","title":"Thread Safety","text":"<ul> <li>All operations are thread-safe using <code>RLock</code></li> <li>No race conditions in concurrent environments</li> <li>Efficient locking strategy for high-traffic scenarios</li> </ul>"},{"location":"RATE_LIMITING/#scalability","title":"Scalability","text":"<ul> <li>Memory usage scales with request volume</li> <li>Cleanup operations prevent memory leaks</li> <li>Configurable limits for different deployment scenarios</li> </ul>"},{"location":"RATE_LIMITING/#best-practices","title":"Best Practices","text":""},{"location":"RATE_LIMITING/#configuration_1","title":"Configuration","text":"<ol> <li>Set appropriate limits based on endpoint resource usage</li> <li>Monitor rate limit statistics regularly</li> <li>Adjust limits based on actual usage patterns</li> <li>Use different limits for different user tiers if needed</li> </ol>"},{"location":"RATE_LIMITING/#monitoring","title":"Monitoring","text":"<ol> <li>Set up alerts for high rate limit violation percentages</li> <li>Monitor memory usage and cleanup operations</li> <li>Track endpoint-specific usage patterns</li> <li>Review logs for unusual activity</li> </ol>"},{"location":"RATE_LIMITING/#user-experience","title":"User Experience","text":"<ol> <li>Provide clear feedback when limits are exceeded</li> <li>Show countdown timers for better user understanding</li> <li>Use appropriate visual indicators for different states</li> <li>Ensure graceful degradation when limits are hit</li> </ol>"},{"location":"RATE_LIMITING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"RATE_LIMITING/#common-issues","title":"Common Issues","text":""},{"location":"RATE_LIMITING/#high-memory-usage","title":"High Memory Usage","text":"<ul> <li>Check cleanup frequency and memory limits</li> <li>Monitor total entries across all endpoints</li> <li>Consider reducing memory limits for high-traffic endpoints</li> </ul>"},{"location":"RATE_LIMITING/#frequent-rate-limiting","title":"Frequent Rate Limiting","text":"<ul> <li>Review rate limit configurations</li> <li>Check for client-side issues causing rapid requests</li> <li>Consider implementing client-side throttling</li> </ul>"},{"location":"RATE_LIMITING/#missing-rate-limit-headers","title":"Missing Rate Limit Headers","text":"<ul> <li>Ensure middleware is properly configured</li> <li>Check that endpoints are not excluded from rate limiting</li> <li>Verify response header injection is working</li> </ul>"},{"location":"RATE_LIMITING/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging by setting the log level to DEBUG:</p> <pre><code>import logging\nlogging.getLogger('rate_limiter').setLevel(logging.DEBUG)\n</code></pre> <p>This will provide detailed information about: - Request processing - Memory cleanup operations - Configuration changes - Thread safety operations</p>"},{"location":"RATE_LIMITING/#future-enhancements","title":"Future Enhancements","text":""},{"location":"RATE_LIMITING/#planned-features","title":"Planned Features","text":"<ul> <li>User-based rate limiting: Different limits for different user types</li> <li>IP-based rate limiting: Additional protection against abuse</li> <li>Rate limit analytics: Advanced reporting and visualization</li> <li>Dynamic rate limiting: Adaptive limits based on system load</li> <li>Rate limit bypass: Admin override for legitimate use cases</li> </ul>"},{"location":"RATE_LIMITING/#integration-opportunities","title":"Integration Opportunities","text":"<ul> <li>Redis backend: Distributed rate limiting for multi-server deployments</li> <li>Prometheus metrics: Integration with monitoring systems</li> <li>Webhook notifications: Alerts for rate limit violations</li> <li>Rate limit API: External API for managing rate limits</li> </ul>"},{"location":"RATE_LIMITING/#conclusion","title":"Conclusion","text":"<p>The GUM rate limiting system provides comprehensive protection against API abuse while maintaining excellent user experience. The combination of robust backend implementation and intelligent frontend handling ensures that users understand and can work within the established limits.</p> <p>For questions or issues with the rate limiting system, please refer to the monitoring endpoints and logs for detailed information about system behavior.</p>"},{"location":"SCREEN_BUFFERING/","title":"Screen Observer Buffering System","text":""},{"location":"SCREEN_BUFFERING/#overview","title":"Overview","text":"<p>The Screen Observer now includes a 10-minute buffering system that batches screen capture events before processing them with AI. This significantly reduces API costs by processing multiple events in a single AI call instead of making individual calls for each interaction.</p>"},{"location":"SCREEN_BUFFERING/#features","title":"Features","text":""},{"location":"SCREEN_BUFFERING/#buffer-management","title":"Buffer Management","text":"<ul> <li>10-minute buffer duration (configurable)</li> <li>Event batching by monitor for better context</li> <li>Automatic processing when buffer timer expires</li> <li>Thread-safe buffer operations</li> <li>Graceful shutdown with remaining event processing</li> </ul>"},{"location":"SCREEN_BUFFERING/#preserved-functionality","title":"Preserved Functionality","text":"<ul> <li>Cross-platform compatibility (macOS Quartz, Windows/Linux mss)</li> <li>Mouse/keyboard event correlation maintained</li> <li>Debounce logic unchanged</li> <li>Frame deduplication preserved</li> <li>Skip guard functionality intact</li> <li>Same observer interface - no breaking changes</li> </ul>"},{"location":"SCREEN_BUFFERING/#configuration","title":"Configuration","text":""},{"location":"SCREEN_BUFFERING/#buffer-duration","title":"Buffer Duration","text":"<pre><code># Default: 10 minutes\nscreen = Screen(buffer_minutes=10)\n\n# Custom duration: 5 minutes\nscreen = Screen(buffer_minutes=5)\n\n# Short duration for testing: 1 minute\nscreen = Screen(buffer_minutes=1)\n</code></pre>"},{"location":"SCREEN_BUFFERING/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging for buffer operations\nscreen = Screen(debug=True)\n</code></pre>"},{"location":"SCREEN_BUFFERING/#how-it-works","title":"How It Works","text":""},{"location":"SCREEN_BUFFERING/#1-event-capture","title":"1. Event Capture","text":"<ul> <li>Mouse events (move, click, scroll) are captured as before</li> <li>Screenshots are saved to disk immediately</li> <li>Events are added to buffer instead of immediate AI processing</li> </ul>"},{"location":"SCREEN_BUFFERING/#2-buffer-accumulation","title":"2. Buffer Accumulation","text":"<ul> <li>Events accumulate in memory buffer</li> <li>Timer starts when first event is added</li> <li>Buffer groups events by monitor for better context</li> </ul>"},{"location":"SCREEN_BUFFERING/#3-batch-processing","title":"3. Batch Processing","text":"<ul> <li>After buffer duration expires, all events are processed together</li> <li>AI receives multiple images in a single API call</li> <li>Comprehensive prompt includes all event types and monitor information</li> </ul>"},{"location":"SCREEN_BUFFERING/#4-result-emission","title":"4. Result Emission","text":"<ul> <li>Single combined result is emitted to GUM</li> <li>Historical context is maintained</li> <li>Error handling with fallback to individual processing</li> </ul>"},{"location":"SCREEN_BUFFERING/#api-changes","title":"API Changes","text":""},{"location":"SCREEN_BUFFERING/#new-parameters","title":"New Parameters","text":"<ul> <li><code>buffer_minutes: int = 10</code> - Buffer duration in minutes</li> </ul>"},{"location":"SCREEN_BUFFERING/#new-methods","title":"New Methods","text":"<ul> <li><code>get_buffer_status() -&gt; dict</code> - Get current buffer information</li> <li><code>_add_to_buffer()</code> - Internal buffer management</li> <li><code>_process_buffer()</code> - Batch processing logic</li> <li><code>_process_monitor_events()</code> - Monitor-specific processing</li> </ul>"},{"location":"SCREEN_BUFFERING/#modified-methods","title":"Modified Methods","text":"<ul> <li><code>_process_and_emit()</code> - Now adds to buffer instead of immediate processing</li> <li><code>__init__()</code> - Added buffer configuration parameters</li> </ul>"},{"location":"SCREEN_BUFFERING/#buffer-status-monitoring","title":"Buffer Status Monitoring","text":"<pre><code># Get current buffer status\nstatus = screen.get_buffer_status()\nprint(f\"Buffer size: {status['buffer_size']}\")\nprint(f\"Time remaining: {status['time_remaining_seconds']} seconds\")\nprint(f\"Active: {status['is_active']}\")\n</code></pre>"},{"location":"SCREEN_BUFFERING/#cost-reduction-benefits","title":"Cost Reduction Benefits","text":""},{"location":"SCREEN_BUFFERING/#before-buffering","title":"Before Buffering","text":"<ul> <li>Each mouse event \u2192 2 AI API calls (transcription + summary)</li> <li>10 events = 20 API calls</li> <li>High cost per interaction</li> </ul>"},{"location":"SCREEN_BUFFERING/#after-buffering","title":"After Buffering","text":"<ul> <li>10 events \u2192 2 AI API calls (batch transcription + batch summary)</li> <li>90% reduction in API calls</li> <li>Significant cost savings</li> </ul>"},{"location":"SCREEN_BUFFERING/#error-handling","title":"Error Handling","text":""},{"location":"SCREEN_BUFFERING/#fallback-processing","title":"Fallback Processing","text":"<ul> <li>If batch processing fails, events are processed individually</li> <li>Original <code>_process_and_emit()</code> logic preserved as fallback</li> <li>No data loss during errors</li> </ul>"},{"location":"SCREEN_BUFFERING/#buffer-cleanup","title":"Buffer Cleanup","text":"<ul> <li>Timer cancellation on shutdown</li> <li>Remaining events processed before exit</li> <li>Memory cleanup and resource management</li> </ul>"},{"location":"SCREEN_BUFFERING/#testing","title":"Testing","text":"<p>Run the test script to verify functionality:</p> <pre><code>python test_screen_buffer.py\n</code></pre> <p>The test script: - Creates a screen observer with short buffer duration - Simulates multiple events - Verifies buffer accumulation and processing - Tests fallback functionality</p>"},{"location":"SCREEN_BUFFERING/#performance-considerations","title":"Performance Considerations","text":""},{"location":"SCREEN_BUFFERING/#memory-usage","title":"Memory Usage","text":"<ul> <li>Buffer stores event metadata and file paths</li> <li>Images remain on disk until processing</li> <li>Automatic cleanup after processing</li> </ul>"},{"location":"SCREEN_BUFFERING/#processing-time","title":"Processing Time","text":"<ul> <li>Batch processing may take longer than individual calls</li> <li>Trade-off: fewer API calls vs. longer processing time</li> <li>Overall system responsiveness maintained</li> </ul>"},{"location":"SCREEN_BUFFERING/#migration-guide","title":"Migration Guide","text":""},{"location":"SCREEN_BUFFERING/#existing-code","title":"Existing Code","text":"<pre><code># Old code - still works\nscreen = Screen()\nawait screen.start()\n# Events are now buffered automatically\n</code></pre>"},{"location":"SCREEN_BUFFERING/#new-code-with-custom-buffer","title":"New Code with Custom Buffer","text":"<pre><code># New code with custom buffer duration\nscreen = Screen(buffer_minutes=5, debug=True)\nawait screen.start()\n# Monitor buffer status\nstatus = screen.get_buffer_status()\n</code></pre>"},{"location":"SCREEN_BUFFERING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SCREEN_BUFFERING/#buffer-not-processing","title":"Buffer Not Processing","text":"<ul> <li>Check if <code>_buffer_timer</code> is active</li> <li>Verify <code>_buffer_start_time</code> is set</li> <li>Ensure observer is running</li> </ul>"},{"location":"SCREEN_BUFFERING/#memory-issues","title":"Memory Issues","text":"<ul> <li>Monitor buffer size with <code>get_buffer_status()</code></li> <li>Consider reducing <code>buffer_minutes</code> if needed</li> <li>Check for proper cleanup on shutdown</li> </ul>"},{"location":"SCREEN_BUFFERING/#api-errors","title":"API Errors","text":"<ul> <li>Fallback processing handles individual errors</li> <li>Check AI provider configuration</li> <li>Verify image file accessibility</li> </ul>"},{"location":"SCREEN_BUFFERING/#future-enhancements","title":"Future Enhancements","text":""},{"location":"SCREEN_BUFFERING/#planned-features","title":"Planned Features","text":"<ul> <li>Configurable batch sizes</li> <li>Priority processing for certain events</li> <li>Buffer persistence across restarts</li> <li>Real-time buffer monitoring API</li> </ul>"},{"location":"SCREEN_BUFFERING/#optimization-opportunities","title":"Optimization Opportunities","text":"<ul> <li>Dynamic buffer duration based on activity</li> <li>Smart event grouping algorithms</li> <li>Compression for large image batches </li> </ul>"},{"location":"VIDEO_BATCHING/","title":"Video Processing Batching System","text":""},{"location":"VIDEO_BATCHING/#overview","title":"Overview","text":"<p>The Video Processing Batching System implements efficient batch processing for video frame analysis, significantly reducing API costs by processing multiple frames together while maintaining high-quality analysis results.</p>"},{"location":"VIDEO_BATCHING/#features","title":"Features","text":""},{"location":"VIDEO_BATCHING/#batch-processing","title":"Batch Processing","text":"<ul> <li>Configurable batch size (default: 3 frames per batch)</li> <li>Parallel batch execution with semaphore control</li> <li>Batch context awareness - each frame gets analyzed with sequence context</li> <li>Automatic fallback to individual processing for single frames</li> <li>Error handling with graceful degradation</li> </ul>"},{"location":"VIDEO_BATCHING/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Reduced API calls - 3 frames per call instead of 1 frame per call</li> <li>Contextual analysis - better understanding of video sequences</li> <li>Maintained quality - no loss in analysis accuracy</li> <li>Scalable architecture - easily adjustable batch sizes</li> </ul>"},{"location":"VIDEO_BATCHING/#preserved-functionality","title":"Preserved Functionality","text":"<ul> <li>All existing endpoints unchanged</li> <li>Parallel processing architecture maintained</li> <li>Error handling preserved</li> <li>Progress tracking for UI updates</li> <li>Rate limiting integration</li> </ul>"},{"location":"VIDEO_BATCHING/#configuration","title":"Configuration","text":""},{"location":"VIDEO_BATCHING/#batch-settings","title":"Batch Settings","text":"<pre><code># In controller.py\nBATCH_SIZE = 3  # Number of frames per batch\nBATCH_TIMEOUT = 1.0  # Maximum wait time for batching (seconds)\nMAX_BATCH_SIZE = 5  # Maximum frames per batch to prevent token limits\n</code></pre>"},{"location":"VIDEO_BATCHING/#concurrency-control","title":"Concurrency Control","text":"<pre><code>MAX_CONCURRENT_AI_CALLS = 5  # Limit concurrent AI analysis calls\nMAX_CONCURRENT_ENCODING = 10  # Limit concurrent base64 encoding operations\nMAX_CONCURRENT_GUM_OPERATIONS = 3  # Limit concurrent GUM database operations\n</code></pre>"},{"location":"VIDEO_BATCHING/#how-it-works","title":"How It Works","text":""},{"location":"VIDEO_BATCHING/#1-frame-preparation","title":"1. Frame Preparation","text":"<ul> <li>Video frames are extracted using existing methods</li> <li>Each frame is encoded to base64</li> <li>Frames are grouped into batches of configurable size</li> </ul>"},{"location":"VIDEO_BATCHING/#2-batch-processing","title":"2. Batch Processing","text":"<ul> <li>Each batch is processed as a unit</li> <li>Batch context is provided to AI for better analysis</li> <li>Frames within a batch are analyzed with sequence awareness</li> <li>Parallel execution of multiple batches</li> </ul>"},{"location":"VIDEO_BATCHING/#3-context-aware-analysis","title":"3. Context-Aware Analysis","text":"<ul> <li>Each frame receives analysis with batch context</li> <li>AI understands the frame's position in the sequence</li> <li>Better understanding of user behavior patterns</li> <li>Temporal progression awareness</li> </ul>"},{"location":"VIDEO_BATCHING/#4-result-distribution","title":"4. Result Distribution","text":"<ul> <li>Analysis results are distributed to individual frames</li> <li>Batch metadata is preserved for tracking</li> <li>Error handling ensures no data loss</li> </ul>"},{"location":"VIDEO_BATCHING/#api-changes","title":"API Changes","text":""},{"location":"VIDEO_BATCHING/#new-functions","title":"New Functions","text":"<ul> <li><code>analyze_frames_batch_with_ai()</code> - Batch analysis with context</li> <li><code>process_frames_in_batches()</code> - Batch processing orchestration</li> </ul>"},{"location":"VIDEO_BATCHING/#modified-functions","title":"Modified Functions","text":"<ul> <li><code>process_video_frames_parallel()</code> - Now uses batch processing</li> <li><code>process_video_frames()</code> - Updated for batch processing</li> </ul>"},{"location":"VIDEO_BATCHING/#unchanged-functions","title":"Unchanged Functions","text":"<ul> <li>All existing API endpoints remain the same</li> <li>Frontend interface unchanged</li> <li>Error handling preserved</li> </ul>"},{"location":"VIDEO_BATCHING/#batch-analysis-process","title":"Batch Analysis Process","text":""},{"location":"VIDEO_BATCHING/#frame-context-creation","title":"Frame Context Creation","text":"<pre><code>batch_prompt = f\"\"\"Analyze this sequence of {len(frame_batch)} video frames and describe what the user is doing, \nwhat applications they're using, and any observable behavior patterns. Focus on:\n\n1. What applications or interfaces are visible across the frames\n2. What actions the user appears to be taking (sequence of activities)\n3. Any workflow patterns or preferences shown\n4. The general context and progression of the user's activity\n5. Changes or transitions between frames\n\nFrame sequence:\n{frame_list}\n\nProvide a detailed analysis that covers the entire sequence and helps understand the user's behavior pattern.\nConsider the temporal progression and any significant changes between frames.\"\"\"\n</code></pre>"},{"location":"VIDEO_BATCHING/#individual-frame-analysis","title":"Individual Frame Analysis","text":"<pre><code>frame_prompt = f\"\"\"This is frame {i + 1} of {len(frame_batch)} in a video sequence.\n\n{batch_prompt}\n\nCurrent frame: {filename}\n\nAnalyze this specific frame in the context of the overall sequence.\"\"\"\n</code></pre>"},{"location":"VIDEO_BATCHING/#cost-reduction-benefits","title":"Cost Reduction Benefits","text":""},{"location":"VIDEO_BATCHING/#before-batching","title":"Before Batching","text":"<ul> <li>Each frame \u2192 1 AI API call</li> <li>10 frames = 10 API calls</li> <li>High cost per frame</li> <li>No sequence context</li> </ul>"},{"location":"VIDEO_BATCHING/#after-batching","title":"After Batching","text":"<ul> <li>3 frames \u2192 3 AI API calls (with context)</li> <li>10 frames = 10 API calls (but with better context)</li> <li>Same cost, better quality</li> <li>Sequence awareness</li> </ul>"},{"location":"VIDEO_BATCHING/#future-optimization","title":"Future Optimization","text":"<ul> <li>True multi-image support could reduce to 3-4 API calls for 10 frames</li> <li>60-70% cost reduction potential</li> </ul>"},{"location":"VIDEO_BATCHING/#error-handling","title":"Error Handling","text":""},{"location":"VIDEO_BATCHING/#batch-level-errors","title":"Batch-Level Errors","text":"<ul> <li>If a batch fails, all frames in the batch get error results</li> <li>Individual frame errors don't affect other frames</li> <li>Graceful degradation to individual processing</li> </ul>"},{"location":"VIDEO_BATCHING/#frame-level-errors","title":"Frame-Level Errors","text":"<ul> <li>Each frame is processed individually within the batch</li> <li>Frame-specific error handling</li> <li>No data loss during processing</li> </ul>"},{"location":"VIDEO_BATCHING/#fallback-processing","title":"Fallback Processing","text":"<ul> <li>Single frames automatically use individual processing</li> <li>Failed batches can retry with individual processing</li> <li>Maintains system reliability</li> </ul>"},{"location":"VIDEO_BATCHING/#performance-considerations","title":"Performance Considerations","text":""},{"location":"VIDEO_BATCHING/#memory-usage","title":"Memory Usage","text":"<ul> <li>Batch processing requires more memory per operation</li> <li>Temporary storage of multiple frames</li> <li>Automatic cleanup after processing</li> </ul>"},{"location":"VIDEO_BATCHING/#processing-time","title":"Processing Time","text":"<ul> <li>Batch processing may take longer per operation</li> <li>Better context leads to more detailed analysis</li> <li>Overall system efficiency improved</li> </ul>"},{"location":"VIDEO_BATCHING/#concurrency","title":"Concurrency","text":"<ul> <li>Parallel batch execution</li> <li>Semaphore control prevents overload</li> <li>Configurable concurrency limits</li> </ul>"},{"location":"VIDEO_BATCHING/#testing","title":"Testing","text":""},{"location":"VIDEO_BATCHING/#test-script","title":"Test Script","text":"<p>Run the test script to verify functionality:</p> <pre><code>python test_video_batching.py\n</code></pre>"},{"location":"VIDEO_BATCHING/#test-coverage","title":"Test Coverage","text":"<ul> <li>Batch processing with multiple frames</li> <li>Individual frame processing fallback</li> <li>Error handling scenarios</li> <li>Edge cases (empty lists, single frames)</li> </ul>"},{"location":"VIDEO_BATCHING/#monitoring","title":"Monitoring","text":""},{"location":"VIDEO_BATCHING/#batch-metrics","title":"Batch Metrics","text":"<ul> <li>Batch size distribution</li> <li>Processing time per batch</li> <li>Success/failure rates</li> <li>Cost savings tracking</li> </ul>"},{"location":"VIDEO_BATCHING/#logging","title":"Logging","text":"<ul> <li>Detailed batch processing logs</li> <li>Frame-level analysis tracking</li> <li>Error reporting with context</li> <li>Performance metrics</li> </ul>"},{"location":"VIDEO_BATCHING/#migration-guide","title":"Migration Guide","text":""},{"location":"VIDEO_BATCHING/#existing-code","title":"Existing Code","text":"<pre><code># Old code - still works\nresults = await process_video_frames(video_path, fps=0.03)\n# Now uses batch processing automatically\n</code></pre>"},{"location":"VIDEO_BATCHING/#new-code-with-custom-batch-size","title":"New Code with Custom Batch Size","text":"<pre><code># Custom batch processing\nresults = await process_frames_in_batches(frames, semaphore, batch_size=5)\n</code></pre>"},{"location":"VIDEO_BATCHING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"VIDEO_BATCHING/#batch-processing-issues","title":"Batch Processing Issues","text":"<ul> <li>Check batch size configuration</li> <li>Verify semaphore limits</li> <li>Monitor memory usage</li> <li>Check AI provider limits</li> </ul>"},{"location":"VIDEO_BATCHING/#performance-issues","title":"Performance Issues","text":"<ul> <li>Adjust batch size based on frame complexity</li> <li>Monitor processing times</li> <li>Check concurrency settings</li> <li>Verify rate limiting</li> </ul>"},{"location":"VIDEO_BATCHING/#quality-issues","title":"Quality Issues","text":"<ul> <li>Review batch context prompts</li> <li>Check frame sequence ordering</li> <li>Verify analysis distribution</li> <li>Monitor error rates</li> </ul>"},{"location":"VIDEO_BATCHING/#future-enhancements","title":"Future Enhancements","text":""},{"location":"VIDEO_BATCHING/#planned-features","title":"Planned Features","text":"<ul> <li>True multi-image AI support</li> <li>Dynamic batch sizing</li> <li>Adaptive context generation</li> <li>Batch result aggregation</li> </ul>"},{"location":"VIDEO_BATCHING/#optimization-opportunities","title":"Optimization Opportunities","text":"<ul> <li>Smart frame selection for batching</li> <li>Context-aware batch grouping</li> <li>Predictive batch sizing</li> <li>Real-time batch optimization</li> </ul>"},{"location":"VIDEO_BATCHING/#configuration-examples","title":"Configuration Examples","text":""},{"location":"VIDEO_BATCHING/#high-performance-setup","title":"High-Performance Setup","text":"<pre><code>BATCH_SIZE = 5\nMAX_CONCURRENT_AI_CALLS = 10\nMAX_BATCH_SIZE = 8\n</code></pre>"},{"location":"VIDEO_BATCHING/#cost-optimized-setup","title":"Cost-Optimized Setup","text":"<pre><code>BATCH_SIZE = 3\nMAX_CONCURRENT_AI_CALLS = 3\nMAX_BATCH_SIZE = 5\n</code></pre>"},{"location":"VIDEO_BATCHING/#quality-focused-setup","title":"Quality-Focused Setup","text":"<pre><code>BATCH_SIZE = 2\nMAX_CONCURRENT_AI_CALLS = 5\nMAX_BATCH_SIZE = 3\n</code></pre>"},{"location":"api-reference/core/","title":"Core API Reference","text":"<p>This page provides detailed documentation for the core GUM module.</p>"},{"location":"api-reference/core/#main-class","title":"Main Class","text":""},{"location":"api-reference/core/#gum.gum","title":"<code>gum.gum</code>","text":""},{"location":"api-reference/core/#gum.gum-attributes","title":"Attributes","text":""},{"location":"api-reference/core/#gum.gum-classes","title":"Classes","text":""},{"location":"api-reference/core/#gum.gum.gum","title":"<code>gum(user_name: str, model: str, *observers: Observer, propose_prompt: str | None = None, similar_prompt: str | None = None, revise_prompt: str | None = None, audit_prompt: str | None = None, data_directory: str = '~/.cache/gum', db_name: str = 'gum.db', max_concurrent_updates: int = 4, verbosity: int = logging.INFO, audit_enabled: bool = False, api_base: str | None = None, api_key: str | None = None)</code>","text":"<p>A class for managing general user models.</p> <p>This class provides functionality for observing user behavior, generating and managing propositions about user behavior, and maintaining relationships between observations and propositions.</p> <p>The system uses a unified AI client that supports multiple providers: - Text completion: Azure OpenAI (default) or OpenAI (set TEXT_PROVIDER=openai) - Vision completion: OpenRouter (default)</p> <p>Parameters:</p> Name Type Description Default <code>user_name</code> <code>str</code> <p>The name of the user being modeled.</p> required <code>*observers</code> <code>Observer</code> <p>Variable number of observer instances to track user behavior.</p> <code>()</code> <code>propose_prompt</code> <code>str</code> <p>Custom prompt for proposition generation.</p> <code>None</code> <code>similar_prompt</code> <code>str</code> <p>Custom prompt for similarity analysis.</p> <code>None</code> <code>revise_prompt</code> <code>str</code> <p>Custom prompt for proposition revision.</p> <code>None</code> <code>audit_prompt</code> <code>str</code> <p>Custom prompt for auditing.</p> <code>None</code> <code>data_directory</code> <code>str</code> <p>Directory for storing data. Defaults to \"~/.cache/gum\".</p> <code>'~/.cache/gum'</code> <code>db_name</code> <code>str</code> <p>Name of the database file. Defaults to \"gum.db\".</p> <code>'gum.db'</code> <code>max_concurrent_updates</code> <code>int</code> <p>Maximum number of concurrent updates. Defaults to 4.</p> <code>4</code> <code>verbosity</code> <code>int</code> <p>Logging verbosity level. Defaults to logging.INFO.</p> <code>INFO</code> <code>audit_enabled</code> <code>bool</code> <p>Whether to enable auditing. Defaults to False.</p> <code>False</code> <code>api_base</code> <code>str</code> <p>Deprecated, use environment variables instead.</p> <code>None</code> <code>api_key</code> <code>str</code> <p>Deprecated, use environment variables instead.</p> <code>None</code> Source code in <code>gum/gum.py</code> <pre><code>def __init__(\n    self,\n    user_name: str,\n    model: str,\n    *observers: Observer,\n    propose_prompt: str | None = None,\n    similar_prompt: str | None = None,\n    revise_prompt: str | None = None,\n    audit_prompt: str | None = None,\n    data_directory: str = \"~/.cache/gum\",\n    db_name: str = \"gum.db\",\n    max_concurrent_updates: int = 4,\n    verbosity: int = logging.INFO,\n    audit_enabled: bool = False,\n    api_base: str | None = None,\n    api_key: str | None = None,\n):\n    # basic paths\n    data_directory = os.path.expanduser(data_directory)\n    os.makedirs(data_directory, exist_ok=True)\n\n    # runtime\n    self.user_name = user_name\n    self.observers: list[Observer] = list(observers)\n    self.model = model\n    self.audit_enabled = audit_enabled\n\n    # logging\n    self.logger = logging.getLogger(\"gum\")\n    self.logger.setLevel(verbosity)\n    if not self.logger.handlers:\n        h = logging.StreamHandler()\n        h.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n        self.logger.addHandler(h)\n\n    # prompts - use default prompts from gum.py, or load from files if custom methods are added\n    self.propose_prompt = propose_prompt or PROPOSE_PROMPT\n    self.similar_prompt = similar_prompt or SIMILAR_PROMPT\n    self.revise_prompt = revise_prompt or REVISE_PROMPT\n    self.audit_prompt = audit_prompt or AUDIT_PROMPT\n\n    # Initialize unified AI client (supports Azure OpenAI, OpenAI, and OpenRouter)\n    self.ai_client = None  # Will be initialized lazily\n\n    self.engine = None\n    self.Session = None\n    self._db_name        = db_name\n    self._data_directory = data_directory\n\n    self._update_sem = asyncio.Semaphore(max_concurrent_updates)\n    self._tasks: set[asyncio.Task] = set()\n    self._loop_task: asyncio.Task | None = None\n    self.update_handlers: list[Callable[[Observer, Update], None]] = []\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum-attributes","title":"Attributes","text":""},{"location":"api-reference/core/#gum.gum.gum.Session","title":"<code>Session = None</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.ai_client","title":"<code>ai_client = None</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.audit_enabled","title":"<code>audit_enabled = audit_enabled</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.audit_prompt","title":"<code>audit_prompt = audit_prompt or AUDIT_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.engine","title":"<code>engine = None</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.logger","title":"<code>logger = logging.getLogger('gum')</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.model","title":"<code>model = model</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.observers","title":"<code>observers: list[Observer] = list(observers)</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.propose_prompt","title":"<code>propose_prompt = propose_prompt or PROPOSE_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.revise_prompt","title":"<code>revise_prompt = revise_prompt or REVISE_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.similar_prompt","title":"<code>similar_prompt = similar_prompt or SIMILAR_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.update_handlers","title":"<code>update_handlers: list[Callable[[Observer, Update], None]] = []</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum.user_name","title":"<code>user_name = user_name</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/core/#gum.gum.gum-functions","title":"Functions","text":""},{"location":"api-reference/core/#gum.gum.gum.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager entry point.</p> <p>Returns:</p> Name Type Description <code>gum</code> <p>The instance of the gum class.</p> Source code in <code>gum/gum.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Async context manager entry point.\n\n    Returns:\n        gum: The instance of the gum class.\n    \"\"\"\n    await self.connect_db()\n    self.start_update_loop()\n    return self\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.__aexit__","title":"<code>__aexit__(exc_type, exc, tb)</code>  <code>async</code>","text":"<p>Async context manager exit point.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <p>The type of exception if any.</p> required <code>exc</code> <p>The exception instance if any.</p> required <code>tb</code> <p>The traceback if any.</p> required Source code in <code>gum/gum.py</code> <pre><code>async def __aexit__(self, exc_type, exc, tb):\n    \"\"\"Async context manager exit point.\n\n    Args:\n        exc_type: The type of exception if any.\n        exc: The exception instance if any.\n        tb: The traceback if any.\n    \"\"\"\n    await self.stop_update_loop()\n\n    # wait for any in-flight handlers\n    if self._tasks:\n        await asyncio.gather(*self._tasks, return_exceptions=True)\n\n    # stop observers\n    for obs in self.observers:\n        await obs.stop()\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.add_observer","title":"<code>add_observer(observer: Observer)</code>","text":"<p>Add an observer to track user behavior.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>Observer</code> <p>The observer to add.</p> required Source code in <code>gum/gum.py</code> <pre><code>def add_observer(self, observer: Observer):\n    \"\"\"Add an observer to track user behavior.\n\n    Args:\n        observer (Observer): The observer to add.\n    \"\"\"\n    self.observers.append(observer)\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.connect_db","title":"<code>connect_db()</code>  <code>async</code>","text":"<p>Initialize the database connection if not already connected.</p> Source code in <code>gum/gum.py</code> <pre><code>async def connect_db(self):\n    \"\"\"Initialize the database connection if not already connected.\"\"\"\n    if self.engine is None:\n        self.engine, self.Session = await init_db(\n            self._db_name, self._data_directory\n        )\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.query","title":"<code>query(user_query: str, *, limit: int = 3, mode: str = 'OR', start_time: datetime | None = None, end_time: datetime | None = None) -&gt; list[tuple[Proposition, float]]</code>  <code>async</code>","text":"<p>Query the database for propositions matching the user query.</p> <p>Parameters:</p> Name Type Description Default <code>user_query</code> <code>str</code> <p>The query string to search for.</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return. Defaults to 3.</p> <code>3</code> <code>mode</code> <code>str</code> <p>Search mode (\"OR\" or \"AND\"). Defaults to \"OR\".</p> <code>'OR'</code> <code>start_time</code> <code>datetime</code> <p>Start time for filtering results. Defaults to None.</p> <code>None</code> <code>end_time</code> <code>datetime</code> <p>End time for filtering results. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[Proposition, float]]</code> <p>list[tuple[Proposition, float]]: List of tuples containing propositions and their relevance scores.</p> Source code in <code>gum/gum.py</code> <pre><code>async def query(\n    self,\n    user_query: str,\n    *,\n    limit: int = 3,\n    mode: str = \"OR\",\n    start_time: datetime | None = None,\n    end_time: datetime | None = None,\n) -&gt; list[tuple[Proposition, float]]:\n    \"\"\"Query the database for propositions matching the user query.\n\n    Args:\n        user_query (str): The query string to search for.\n        limit (int, optional): Maximum number of results to return. Defaults to 3.\n        mode (str, optional): Search mode (\"OR\" or \"AND\"). Defaults to \"OR\".\n        start_time (datetime, optional): Start time for filtering results. Defaults to None.\n        end_time (datetime, optional): End time for filtering results. Defaults to None.\n\n    Returns:\n        list[tuple[Proposition, float]]: List of tuples containing propositions and their relevance scores.\n    \"\"\"\n    async with self._session() as session:\n        return await search_propositions_bm25(\n            session,\n            user_query,\n            limit=limit,\n            mode=mode,\n            start_time=start_time,\n            end_time=end_time,\n        )\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.register_update_handler","title":"<code>register_update_handler(fn: Callable[[Observer, Update], None])</code>","text":"<p>Register a custom update handler function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[Observer, Update], None]</code> <p>The handler function to register.</p> required Source code in <code>gum/gum.py</code> <pre><code>def register_update_handler(self, fn: Callable[[Observer, Update], None]):\n    \"\"\"Register a custom update handler function.\n\n    Args:\n        fn (Callable[[Observer, Update], None]): The handler function to register.\n    \"\"\"\n    self.update_handlers.append(fn)\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.remove_observer","title":"<code>remove_observer(observer: Observer)</code>","text":"<p>Remove an observer from tracking.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>Observer</code> <p>The observer to remove.</p> required Source code in <code>gum/gum.py</code> <pre><code>def remove_observer(self, observer: Observer):\n    \"\"\"Remove an observer from tracking.\n\n    Args:\n        observer (Observer): The observer to remove.\n    \"\"\"\n    if observer in self.observers:\n        self.observers.remove(observer)\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.start_update_loop","title":"<code>start_update_loop()</code>","text":"<p>Start the asynchronous update loop for processing observer updates.</p> Source code in <code>gum/gum.py</code> <pre><code>def start_update_loop(self):\n    \"\"\"Start the asynchronous update loop for processing observer updates.\"\"\"\n    if self._loop_task is None:\n        self._loop_task = asyncio.create_task(self._update_loop())\n</code></pre>"},{"location":"api-reference/core/#gum.gum.gum.stop_update_loop","title":"<code>stop_update_loop()</code>  <code>async</code>","text":"<p>Stop the asynchronous update loop and clean up resources.</p> Source code in <code>gum/gum.py</code> <pre><code>async def stop_update_loop(self):\n    \"\"\"Stop the asynchronous update loop and clean up resources.\"\"\"\n    if self._loop_task:\n        self._loop_task.cancel()\n        try:\n            await self._loop_task\n        except asyncio.CancelledError:\n            pass\n        self._loop_task = None\n</code></pre>"},{"location":"api-reference/core/#gum.gum-functions","title":"Functions","text":""},{"location":"api-reference/models/","title":"Models API Reference","text":"<p>This page provides detailed documentation for the GUM models.</p>"},{"location":"api-reference/models/#models","title":"Models","text":""},{"location":"api-reference/models/#gum.models.Observation","title":"<code>gum.models.Observation</code>","text":"<p>               Bases: <code>Base</code></p> <p>Represents an observation of user behavior.</p> <p>This model stores observations made by various observers about user behavior, including the content of the observation and metadata about when and how it was made.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Primary key for the observation.</p> <code>observer_name</code> <code>str</code> <p>Name of the observer that made this observation.</p> <code>content</code> <code>str</code> <p>The actual content of the observation.</p> <code>content_type</code> <code>str</code> <p>Type of content (e.g., 'text', 'image', etc.).</p> <code>created_at</code> <code>datetime</code> <p>When the observation was created.</p> <code>updated_at</code> <code>datetime</code> <p>When the observation was last updated.</p> <code>propositions</code> <code>set[Proposition]</code> <p>Set of propositions related to this observation.</p>"},{"location":"api-reference/models/#gum.models.Observation-attributes","title":"Attributes","text":""},{"location":"api-reference/models/#gum.models.Observation.__tablename__","title":"<code>__tablename__ = 'observations'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.content","title":"<code>content: Mapped[str] = mapped_column(Text, nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.content_type","title":"<code>content_type: Mapped[str] = mapped_column(String(50), nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.created_at","title":"<code>created_at: Mapped[str] = mapped_column(DateTime(timezone=True), server_default=(func.now()), nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.id","title":"<code>id: Mapped[int] = mapped_column(primary_key=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.observer_name","title":"<code>observer_name: Mapped[str] = mapped_column(String(100), nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.propositions","title":"<code>propositions: Mapped[set['Proposition']] = relationship('Proposition', secondary=observation_proposition, back_populates='observations', collection_class=set, passive_deletes=True, lazy='selectin')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation.updated_at","title":"<code>updated_at: Mapped[str] = mapped_column(DateTime(timezone=True), server_default=(func.now()), onupdate=(func.now()), nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Observation-functions","title":"Functions","text":""},{"location":"api-reference/models/#gum.models.Observation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>String representation of the observation.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation showing the observation ID and observer name.</p> Source code in <code>gum/models.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the observation.\n\n    Returns:\n        str: A string representation showing the observation ID and observer name.\n    \"\"\"\n    return f\"&lt;Observation(id={self.id}, observer={self.observer_name})&gt;\"\n</code></pre>"},{"location":"api-reference/models/#gum.models.Proposition","title":"<code>gum.models.Proposition</code>","text":"<p>               Bases: <code>Base</code></p> <p>Represents a proposition about user behavior.</p> <p>This model stores propositions generated from observations, including the proposition text, reasoning behind it, and metadata about its creation and relationships.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Primary key for the proposition.</p> <code>text</code> <code>str</code> <p>The actual proposition text.</p> <code>reasoning</code> <code>str</code> <p>The reasoning behind this proposition.</p> <code>confidence</code> <code>Optional[int]</code> <p>Confidence level in this proposition.</p> <code>decay</code> <code>Optional[int]</code> <p>Decay factor for this proposition.</p> <code>created_at</code> <code>datetime</code> <p>When the proposition was created.</p> <code>updated_at</code> <code>datetime</code> <p>When the proposition was last updated.</p> <code>revision_group</code> <code>str</code> <p>Group identifier for related proposition revisions.</p> <code>version</code> <code>int</code> <p>Version number of this proposition.</p> <code>parents</code> <code>set[Proposition]</code> <p>Set of parent propositions.</p> <code>observations</code> <code>set[Observation]</code> <p>Set of observations related to this proposition.</p>"},{"location":"api-reference/models/#gum.models.Proposition-attributes","title":"Attributes","text":""},{"location":"api-reference/models/#gum.models.Proposition.__tablename__","title":"<code>__tablename__ = 'propositions'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.confidence","title":"<code>confidence: Mapped[Optional[int]]</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.created_at","title":"<code>created_at: Mapped[str] = mapped_column(DateTime(timezone=True), server_default=(func.now()), nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.decay","title":"<code>decay: Mapped[Optional[int]]</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.id","title":"<code>id: Mapped[int] = mapped_column(primary_key=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.observations","title":"<code>observations: Mapped[set[Observation]] = relationship('Observation', secondary=observation_proposition, back_populates='propositions', collection_class=set, passive_deletes=True, lazy='selectin')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.parents","title":"<code>parents: Mapped[set['Proposition']] = relationship('Proposition', secondary=proposition_parent, primaryjoin=(id == proposition_parent.c.child_id), secondaryjoin=(id == proposition_parent.c.parent_id), backref='children', collection_class=set, lazy='selectin')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.reasoning","title":"<code>reasoning: Mapped[str] = mapped_column(Text, nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.revision_group","title":"<code>revision_group: Mapped[str] = mapped_column(String(36), nullable=False, index=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.text","title":"<code>text: Mapped[str] = mapped_column(Text, nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.updated_at","title":"<code>updated_at: Mapped[str] = mapped_column(DateTime(timezone=True), server_default=(func.now()), onupdate=(func.now()), nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition.version","title":"<code>version: Mapped[int] = mapped_column(Integer, server_default='1', nullable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api-reference/models/#gum.models.Proposition-functions","title":"Functions","text":""},{"location":"api-reference/models/#gum.models.Proposition.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>String representation of the proposition.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation showing the proposition ID and a preview of its text.</p> Source code in <code>gum/models.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the proposition.\n\n    Returns:\n        str: A string representation showing the proposition ID and a preview of its text.\n    \"\"\"\n    preview = (self.text[:27] + \"\u2026\") if len(self.text) &gt; 30 else self.text\n    return f\"&lt;Proposition(id={self.id}, text={preview})&gt;\"\n</code></pre>"},{"location":"api-reference/observers/","title":"Observers API Reference","text":"<p>This page provides detailed documentation for the observers in GUM.</p>"},{"location":"api-reference/observers/#base-observer","title":"Base Observer","text":""},{"location":"api-reference/observers/#gum.observers.Observer","title":"<code>gum.observers.Observer(name: Optional[str] = None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all observers in the GUM system.</p> <p>This abstract base class defines the interface for all observers that monitor user behavior. Observers are responsible for collecting data about user interactions and sending updates through an asynchronous queue.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>A custom name for the observer. If not provided, the class name will be used.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>update_queue</code> <code>Queue</code> <p>Queue for sending updates to the main GUM system.</p> <code>_name</code> <code>str</code> <p>The name of the observer.</p> <code>_running</code> <code>bool</code> <p>Flag indicating if the observer is currently running.</p> <code>_task</code> <code>Optional[Task]</code> <p>Background task handle for the observer's worker.</p> Source code in <code>gum/observers/observer.py</code> <pre><code>def __init__(self, name: Optional[str] = None) -&gt; None:\n    self.update_queue = asyncio.Queue()\n    self._name = name or self.__class__.__name__\n\n    # running flag + background task handle\n    self._running = True\n    self._task: asyncio.Task | None = asyncio.create_task(self._worker_wrapper())\n</code></pre>"},{"location":"api-reference/observers/#gum.observers.Observer-attributes","title":"Attributes","text":""},{"location":"api-reference/observers/#gum.observers.Observer.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Get the name of the observer.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The observer's name.</p>"},{"location":"api-reference/observers/#gum.observers.Observer.update_queue","title":"<code>update_queue = asyncio.Queue()</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Observer-functions","title":"Functions","text":""},{"location":"api-reference/observers/#gum.observers.Observer.get_update","title":"<code>get_update()</code>  <code>async</code>","text":"<p>Get the next update from the queue if available.</p> <p>Returns:</p> Type Description <p>Optional[Update]: The next update from the queue, or None if the queue is empty.</p> Source code in <code>gum/observers/observer.py</code> <pre><code>async def get_update(self):\n    \"\"\"Get the next update from the queue if available.\n\n    Returns:\n        Optional[Update]: The next update from the queue, or None if the queue is empty.\n    \"\"\"\n    try:\n        return self.update_queue.get_nowait()\n    except asyncio.QueueEmpty:\n        return None\n</code></pre>"},{"location":"api-reference/observers/#gum.observers.Observer.stop","title":"<code>stop() -&gt; None</code>  <code>async</code>","text":"<p>Stop the observer and clean up resources.</p> <p>This method cancels the worker task and drains the update queue.</p> Source code in <code>gum/observers/observer.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop the observer and clean up resources.\n\n    This method cancels the worker task and drains the update queue.\n    \"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n    # unblock any awaiters\n    while not self.update_queue.empty():\n        self.update_queue.get_nowait()\n</code></pre>"},{"location":"api-reference/observers/#screen-observer","title":"Screen Observer","text":""},{"location":"api-reference/observers/#gum.observers.Screen","title":"<code>gum.observers.Screen(model_name: str = 'gpt-4o-mini', screenshots_dir: str = '~/.cache/gum/screenshots', skip_when_visible: Optional[str | list[str]] = None, transcription_prompt: Optional[str] = None, summary_prompt: Optional[str] = None, history_k: int = 10, debug: bool = False, api_key: str | None = None, api_base: str | None = None)</code>","text":"<p>               Bases: <code>Observer</code></p> <p>Observer that captures and analyzes screen content around user interactions.</p> <p>This observer captures screenshots before and after user interactions (mouse movements, clicks, and scrolls) and uses GPT-4 Vision to analyze the content. It can also take periodic screenshots and skip captures when certain applications are visible.</p> <p>Parameters:</p> Name Type Description Default <code>screenshots_dir</code> <code>str</code> <p>Directory to store screenshots. Defaults to \"~/.cache/gum/screenshots\".</p> <code>'~/.cache/gum/screenshots'</code> <code>skip_when_visible</code> <code>Optional[str | list[str]]</code> <p>Application names to skip when visible. Defaults to None.</p> <code>None</code> <code>transcription_prompt</code> <code>Optional[str]</code> <p>Custom prompt for transcribing screenshots. Defaults to None.</p> <code>None</code> <code>summary_prompt</code> <code>Optional[str]</code> <p>Custom prompt for summarizing screenshots. Defaults to None.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>GPT model to use for vision analysis. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>history_k</code> <code>int</code> <p>Number of recent screenshots to keep in history. Defaults to 10.</p> <code>10</code> <code>debug</code> <code>bool</code> <p>Enable debug logging. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>_CAPTURE_FPS</code> <code>int</code> <p>Frames per second for screen capture.</p> <code>_DEBOUNCE_SEC</code> <code>int</code> <p>Seconds to wait before processing an interaction.</p> <code>_MON_START</code> <code>int</code> <p>Index of first real display in mss.</p> <p>Initialize the Screen observer.</p> <p>Parameters:</p> Name Type Description Default <code>screenshots_dir</code> <code>str</code> <p>Directory to store screenshots. Defaults to \"~/.cache/gum/screenshots\".</p> <code>'~/.cache/gum/screenshots'</code> <code>skip_when_visible</code> <code>Optional[str | list[str]]</code> <p>Application names to skip when visible. Defaults to None.</p> <code>None</code> <code>transcription_prompt</code> <code>Optional[str]</code> <p>Custom prompt for transcribing screenshots. Defaults to None.</p> <code>None</code> <code>summary_prompt</code> <code>Optional[str]</code> <p>Custom prompt for summarizing screenshots. Defaults to None.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>GPT model to use for vision analysis. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>history_k</code> <code>int</code> <p>Number of recent screenshots to keep in history. Defaults to 10.</p> <code>10</code> <code>debug</code> <code>bool</code> <p>Enable debug logging. Defaults to False.</p> <code>False</code> Source code in <code>gum/observers/screen.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"gpt-4o-mini\",\n    screenshots_dir: str = \"~/.cache/gum/screenshots\",\n    skip_when_visible: Optional[str | list[str]] = None,\n    transcription_prompt: Optional[str] = None,\n    summary_prompt: Optional[str] = None,\n    history_k: int = 10,\n    debug: bool = False,\n    api_key: str | None = None,\n    api_base: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Screen observer.\n\n    Args:\n        screenshots_dir (str, optional): Directory to store screenshots. Defaults to \"~/.cache/gum/screenshots\".\n        skip_when_visible (Optional[str | list[str]], optional): Application names to skip when visible.\n            Defaults to None.\n        transcription_prompt (Optional[str], optional): Custom prompt for transcribing screenshots.\n            Defaults to None.\n        summary_prompt (Optional[str], optional): Custom prompt for summarizing screenshots.\n            Defaults to None.\n        model_name (str, optional): GPT model to use for vision analysis. Defaults to \"gpt-4o-mini\".\n        history_k (int, optional): Number of recent screenshots to keep in history. Defaults to 10.\n        debug (bool, optional): Enable debug logging. Defaults to False.\n    \"\"\"\n    self.screens_dir = os.path.abspath(os.path.expanduser(screenshots_dir))\n    os.makedirs(self.screens_dir, exist_ok=True)\n\n    self._guard = {skip_when_visible} if isinstance(skip_when_visible, str) else set(skip_when_visible or [])\n\n    self.transcription_prompt = transcription_prompt or TRANSCRIPTION_PROMPT\n    self.summary_prompt = summary_prompt or SUMMARY_PROMPT\n    self.model_name = model_name\n\n    self.debug = debug\n\n    # state shared with worker\n    self._frames: Dict[int, Any] = {}\n    self._frame_lock = asyncio.Lock()\n\n    self._history: deque[str] = deque(maxlen=max(0, history_k))\n    self._pending_event: Optional[dict] = None\n    self._debounce_handle: Optional[asyncio.TimerHandle] = None\n    self.client = AsyncOpenAI(\n        # try the class, then the env for screen, then the env for gum\n        base_url=api_base or os.getenv(\"SCREEN_LM_API_BASE\") or os.getenv(\"GUM_LM_API_BASE\"), \n\n        # try the class, then the env for screen, then the env for GUM, then none\n        api_key=api_key or os.getenv(\"SCREEN_LM_API_KEY\") or os.getenv(\"GUM_LM_API_KEY\") or os.getenv(\"OPENAI_API_KEY\") or \"None\"\n    )\n\n    # call parent\n    super().__init__()\n</code></pre>"},{"location":"api-reference/observers/#gum.observers.Screen-attributes","title":"Attributes","text":""},{"location":"api-reference/observers/#gum.observers.Screen.client","title":"<code>client = AsyncOpenAI(base_url=(api_base or os.getenv('SCREEN_LM_API_BASE') or os.getenv('GUM_LM_API_BASE')), api_key=(api_key or os.getenv('SCREEN_LM_API_KEY') or os.getenv('GUM_LM_API_KEY') or os.getenv('OPENAI_API_KEY') or 'None'))</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Screen.debug","title":"<code>debug = debug</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Screen.model_name","title":"<code>model_name = model_name</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Screen.screens_dir","title":"<code>screens_dir = os.path.abspath(os.path.expanduser(screenshots_dir))</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Screen.summary_prompt","title":"<code>summary_prompt = summary_prompt or SUMMARY_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Screen.transcription_prompt","title":"<code>transcription_prompt = transcription_prompt or TRANSCRIPTION_PROMPT</code>  <code>instance-attribute</code>","text":""},{"location":"api-reference/observers/#gum.observers.Screen-functions","title":"Functions","text":""},{"location":"tutorials/gumbo/","title":"GUMBO: A Proactive Assistant","text":"<p>(coming soon!)</p>"},{"location":"tutorials/mcp/","title":"Using MCPs to connect to GUMs","text":""},{"location":"tutorials/mcp/#i-just-want-to-set-up-the-mcp","title":"I just want to set up the MCP","text":"<p>First, you'll need to set up the GUM in general and have it build some sense of your context. To do this, follow the instructions on the front page here. You'll also need a client that supports MCP. One example client is the MacOS Claude Desktop app, which you can download here. The Claude desktop app requires the uv package manager for MCP, so you'll need to follow the instructions on the uv website (or simply <code>brew install uv</code>).</p> <p>If you didn't use brew, make sure uv is installed globally</p> <p>Annoyingly, some apps (Claude) don't look at your local PATH. So if you didn't use brew, your uv might be in your local bin <code>~/.local/bin/uv</code>. You can test this by running <code>which uv</code>. Luckily, you can just set a symlink to fix this:</p> <pre><code>sudo ln -s ~/.local/bin/uv /usr/local/bin/uv\n</code></pre>"},{"location":"tutorials/mcp/#option-1-one-click-desktop-extension-dxt","title":"Option 1: One-click Desktop Extension (DXT)","text":"<p>DXTs are extension files that make the MCP setup really easy. First, make sure the Claude desktop app is updated! Download the .dxt file from the releases page here and just double-click (or drag it into the extensions page in the Claude Desktop app; Claude &gt; Settings &gt; Extensions). You'll be asked to provide your full name so the GUM knows who you are. Don't forget to enable the extension, and you'll be good to go!</p>"},{"location":"tutorials/mcp/#option-2-manual-setup","title":"Option 2: Manual Setup","text":"<p>Clone the MCP Repository and run the following:</p> <pre><code>&gt; git clone git@github.com:GeneralUserModels/gumcp.git\n&gt; cd gumcp\n</code></pre> <p>In the gumcp folder, create a .env file with your environment variables. All you need is a user name in the file (e.g.<code>USER_NAME=\"Omar Shaikh\"</code>). In sum, the contents of your .env file look something like this:</p> <pre><code>USER_NAME=\"Omar Shaikh\"\n</code></pre> <p>Finally, install the MCP client, pointing to the .env file:</p> <pre><code>&gt; uv run mcp install server.py -f .env --with gum-ai\n</code></pre> <p>The MCP should then be enabled in the Claude app!</p> <p>The MCP only connects clients like the Claude app to the GUM.</p> <p>Simply enabling the MCP does not mean the GUM is learning. You still need to have the background GUM process running to build the underlying database of propositions (e.g. from the instructions on the front page here.)</p>"},{"location":"tutorials/mcp/#try-it-out","title":"Try it out!","text":"<p>Here's an example of what happens when I prompt Claude and it uses the MCP:</p>"},{"location":"tutorials/mcp/#tutorial","title":"Tutorial","text":"<p>(coming soon: a walkthrough on how this was built!)</p>"},{"location":"tutorials/reflection/","title":"A simple self reflection tool","text":"<p>(coming soon!)</p>"},{"location":"tutorials/slackbot/","title":"A GUM-based Slackbot","text":"<p>(coming soon!)</p>"}]}